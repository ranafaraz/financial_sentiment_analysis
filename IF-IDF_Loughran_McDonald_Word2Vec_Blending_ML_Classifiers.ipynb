{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ranafaraz/financial_sentiment_analysis/blob/main/IF-IDF_Loughran_McDonald_Word2Vec_Blending_ML_Classifiers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtK82S9batdu",
        "outputId": "2372c334-2ca6-420d-adc9-6049346e1464"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/Drive; to attempt to forcibly remount, call drive.mount(\"/content/Drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/Drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Fe9WUxpbbpQK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5e6c5b5-5d81-4ef8-9524-a95d019cef3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: num2words in /usr/local/lib/python3.10/dist-packages (0.5.12)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from num2words) (0.6.2)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import psutil\n",
        "import platform\n",
        "import scipy.sparse as sp\n",
        "from scipy.sparse import csr_matrix, hstack\n",
        "# import joblib\n",
        "# from itertools import permutations\n",
        "from itertools import combinations, chain\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import pytz # timezone\n",
        "from datetime import datetime\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sys\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from tqdm import tqdm\n",
        "import multiprocessing\n",
        "from multiprocessing import Process, Queue\n",
        "\n",
        "\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, cohen_kappa_score\n",
        "# from sklearn.base import clone\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "# from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "!pip install num2words\n",
        "from num2words import num2words\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zYLFMIwAVZk4"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/Drive/MyDrive/python/data.csv\")\n",
        "# custom_stopwords_df = pd.read_csv(\"/content/Drive/MyDrive/python/financial_sentiment_analysis/custom_stopwords.csv\")  # StopWord List\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Python module that will load the Loughran-McDonald Master and its components (and optionally separate sentiment dictionaries).\n",
        "\n",
        "But this technique doesn't worked well in our case. It only added few new features."
      ],
      "metadata": {
        "id": "xGX-KhJA3R08"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AAZVL9xBgOJO"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "Routine to load master dicitonary\n",
        "Version for LM 2020 Master Dictionary\n",
        "\n",
        "Bill McDonald\n",
        "Date: 201510 Updated: 202201\n",
        "\"\"\"\n",
        "\n",
        "import datetime as dt\n",
        "import sys\n",
        "\n",
        "\n",
        "def load_masterdictionary(file_path, print_flag=False, f_log=None, get_other=False):\n",
        "    start_local = dt.datetime.now()\n",
        "    # Setup dictionaries\n",
        "    _master_dictionary = {}\n",
        "    _sentiment_categories = ['negative', 'positive', 'uncertainty', 'litigious',\n",
        "                             'strong_modal', 'weak_modal', 'constraining']\n",
        "    _sentiment_dictionaries = dict()\n",
        "    for sentiment in _sentiment_categories:\n",
        "        _sentiment_dictionaries[sentiment] = dict()\n",
        "\n",
        "    # Load slightly modified common stopwords.\n",
        "    # Dropped from traditional: A, I, S, T, DON, WILL, AGAINST\n",
        "    # Added: AMONG\n",
        "    _stopwords = ['ME', 'MY', 'MYSELF', 'WE', 'OUR', 'OURS', 'OURSELVES', 'YOU', 'YOUR', 'YOURS',\n",
        "                  'YOURSELF', 'YOURSELVES', 'HE', 'HIM', 'HIS', 'HIMSELF', 'SHE', 'HER', 'HERS', 'HERSELF',\n",
        "                  'IT', 'ITS', 'ITSELF', 'THEY', 'THEM', 'THEIR', 'THEIRS', 'THEMSELVES', 'WHAT', 'WHICH',\n",
        "                  'WHO', 'WHOM', 'THIS', 'THAT', 'THESE', 'THOSE', 'AM', 'IS', 'ARE', 'WAS', 'WERE', 'BE',\n",
        "                  'BEEN', 'BEING', 'HAVE', 'HAS', 'HAD', 'HAVING', 'DO', 'DOES', 'DID', 'DOING', 'AN',\n",
        "                  'THE', 'AND', 'BUT', 'IF', 'OR', 'BECAUSE', 'AS', 'UNTIL', 'WHILE', 'OF', 'AT', 'BY',\n",
        "                  'FOR', 'WITH', 'ABOUT', 'BETWEEN', 'INTO', 'THROUGH', 'DURING', 'BEFORE',\n",
        "                  'AFTER', 'ABOVE', 'BELOW', 'TO', 'FROM', 'UP', 'DOWN', 'IN', 'OUT', 'ON', 'OFF', 'OVER',\n",
        "                  'UNDER', 'AGAIN', 'FURTHER', 'THEN', 'ONCE', 'HERE', 'THERE', 'WHEN', 'WHERE', 'WHY',\n",
        "                  'HOW', 'ALL', 'ANY', 'BOTH', 'EACH', 'FEW', 'MORE', 'MOST', 'OTHER', 'SOME', 'SUCH',\n",
        "                  'NO', 'NOR', 'NOT', 'ONLY', 'OWN', 'SAME', 'SO', 'THAN', 'TOO', 'VERY', 'CAN',\n",
        "                  'JUST', 'SHOULD', 'NOW', 'AMONG']\n",
        "\n",
        "    # Loop thru words and load dictionaries\n",
        "    with open(file_path) as f:\n",
        "        _total_documents = 0\n",
        "        _md_header = f.readline()  # Consume header line\n",
        "        print()\n",
        "        for line in f:\n",
        "            cols = line.rstrip('\\n').split(',')\n",
        "            word = cols[0]\n",
        "            _master_dictionary[word] = MasterDictionary(cols, _stopwords)\n",
        "            for sentiment in _sentiment_categories:\n",
        "                if getattr(_master_dictionary[word], sentiment):\n",
        "                    _sentiment_dictionaries[sentiment][word] = 0\n",
        "            _total_documents += _master_dictionary[cols[0]].doc_count\n",
        "            if len(_master_dictionary) % 5000 == 0 and print_flag:\n",
        "                print(f'\\r ...Loading Master Dictionary {len(_master_dictionary):,}', end='', flush=True)\n",
        "\n",
        "    if print_flag:\n",
        "        print('\\r', end='')  # clear line\n",
        "        print(f'\\nMaster Dictionary loaded from file:\\n  {file_path}\\n')\n",
        "        print(f'  master_dictionary has {len(_master_dictionary):,} words.\\n')\n",
        "\n",
        "    if f_log:\n",
        "        try:\n",
        "            f_log.write('\\n\\n  FUNCTION: load_masterdictionary' +\n",
        "                        '(file_path, print_flag, f_log, get_other)\\n')\n",
        "            f_log.write(f'\\n    file_path  = {file_path}')\n",
        "            f_log.write(f'\\n    print_flag = {print_flag}')\n",
        "            f_log.write(f'\\n    f_log      = {f_log.name}')\n",
        "            f_log.write(f'\\n    get_other  = {get_other}')\n",
        "            f_log.write(f'\\n\\n    {len(_master_dictionary):,} words loaded in master_dictionary.\\n')\n",
        "            f_log.write(f'\\n    Sentiment:')\n",
        "            for sentiment in _sentiment_categories:\n",
        "                f_log.write(f'\\n      {sentiment:13}: {len(_sentiment_dictionaries[sentiment]):8,}')\n",
        "            f_log.write(f'\\n\\n  END FUNCTION: load_masterdictionary: {(dt.datetime.now()-start_local)}')\n",
        "        except Exception as e:\n",
        "            print('Log file in load_masterdictionary is not available for writing')\n",
        "            print(f'Error = {e}')\n",
        "\n",
        "    if get_other:\n",
        "        return _master_dictionary, _md_header, _sentiment_categories, _sentiment_dictionaries, _stopwords, _total_documents\n",
        "    else:\n",
        "        return _master_dictionary\n",
        "\n",
        "\n",
        "class MasterDictionary:\n",
        "    def __init__(self, cols, _stopwords):\n",
        "        for ptr, col in enumerate(cols):\n",
        "            if col == '':\n",
        "                cols[ptr] = '0'\n",
        "        try:\n",
        "            self.word = cols[0].upper()\n",
        "            self.sequence_number = int(cols[1])\n",
        "            self.word_count = int(cols[2])\n",
        "            self.word_proportion = float(cols[3])\n",
        "            self.average_proportion = float(cols[4])\n",
        "            self.std_dev_prop = float(cols[5])\n",
        "            self.doc_count = int(cols[6])\n",
        "            self.negative = int(cols[7])\n",
        "            self.positive = int(cols[8])\n",
        "            self.uncertainty = int(cols[9])\n",
        "            self.litigious = int(cols[10])\n",
        "            self.strong_modal = int(cols[11])\n",
        "            self.weak_modal = int(cols[12])\n",
        "            self.constraining = int(cols[13])\n",
        "            self.syllables = int(cols[14])\n",
        "            self.source = cols[15]\n",
        "            if self.word in _stopwords:\n",
        "                self.stopword = True\n",
        "            else:\n",
        "                self.stopword = False\n",
        "        except:\n",
        "            print('ERROR in class MasterDictionary')\n",
        "            print(f'word = {cols[0]} : seqnum = {cols[1]}')\n",
        "            quit()\n",
        "        return\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the method to load Loughran-McDonald Master Dictionary\n",
        "if __name__ == '__main__':\n",
        "    start = dt.datetime.now()\n",
        "    print(f'\\n\\n{start.strftime(\"%c\")}\\nPROGRAM NAME: {sys.argv[0]}\\n')\n",
        "    f_log = open('/content/Drive/MyDrive/python/financial_sentiment_analysis/Load_MD_Logfile.txt', 'w')\n",
        "    md = (r\"/content/Drive/MyDrive/python/financial_sentiment_analysis/Loughran-McDonald_MasterDictionary_1993-2021.csv\")\n",
        "    master_dictionary, md_header, sentiment_categories, sentiment_dictionaries, stopwords, total_documents = \\\n",
        "        load_masterdictionary(md, True, f_log, True)\n",
        "    print(f'\\n\\nRuntime: {(dt.datetime.now()-start)}')\n",
        "    print(f'\\nNormal termination.\\n{dt.datetime.now().strftime(\"%c\")}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MW9ZzBB9QEHt",
        "outputId": "cef9547d-6961-4015-f798-5cbed1b24bf8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Sat Sep 30 07:21:21 2023\n",
            "PROGRAM NAME: /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\n",
            "\n",
            "\n",
            " ...Loading Master Dictionary 85,000\n",
            "Master Dictionary loaded from file:\n",
            "  /content/Drive/MyDrive/python/financial_sentiment_analysis/Loughran-McDonald_MasterDictionary_1993-2021.csv\n",
            "\n",
            "  master_dictionary has 86,531 words.\n",
            "\n",
            "\n",
            "\n",
            "Runtime: 0:00:03.803402\n",
            "\n",
            "Normal termination.\n",
            "Sat Sep 30 07:21:25 2023\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subword tokenization using libraries like SentencePiece or Byte Pair Encoding (BPE) is useful when you want to tokenize text into subword units. This can be particularly helpful for languages with complex morphology or when dealing with out-of-vocabulary words. Here's how you can use these libraries:\n",
        "\n",
        "Using SentencePiece:\n",
        "SentencePiece is a popular subword tokenization library developed by Google. It's used for a wide range of natural language processing tasks.\n",
        "\n",
        "But this technique doesn't worked in our case. Accuracy and number of features fell dramatically, as soon as we applied it."
      ],
      "metadata": {
        "id": "HeoYkeAH3rLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a SentencePiece model for Tokenization\n",
        "!pip install sentencepiece\n",
        "import sentencepiece as spm\n",
        "spm.SentencePieceTrainer.train(input=(r\"/content/Drive/MyDrive/python/data.csv\"), model_prefix='mymodel', vocab_size=10000) # vocab_size=5000\n",
        "sp = spm.SentencePieceProcessor(model_file='mymodel.model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqe6zVVwPeM8",
        "outputId": "ba24aadc-5288-42d5-803a-bc7031eec6c4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "A8NsKSDV4q12"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2wklQSHh4xQv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell is dedicated for preprocessing and tokenization processes."
      ],
      "metadata": {
        "id": "LMb_h3O3fu0K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_oqnzngiVoA4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Custom Tokenization function\n",
        "def custom_tokenize(text):\n",
        "    # Regular expressions to tokenize financial-specific terms and symbols\n",
        "    # Example: Tokenize stock tickers like AAPL as single tokens\n",
        "    text = re.sub(r'([A-Z]+)', r'\\1 ', text)  # Add space after uppercase letters\n",
        "\n",
        "    # Example: Tokenize currency symbols (e.g., $, €) and percentages\n",
        "    text = re.sub(r'([$€%])', r' \\1 ', text)  # Convert currency symbols and percentages to standardized forms\n",
        "\n",
        "    # Tokenize numbers and symbols separately, and keep decimal numbers intact\n",
        "    text = re.sub(r'(\\d+\\.\\d+|\\d+)', r' \\1 ', text)  # Separate numbers and decimals\n",
        "    text = re.sub(r'([$€%])', r' \\1 ', text)  # Separate currency symbols and percentages\n",
        "\n",
        "    # Tokenize the text into words\n",
        "    words = text.split()\n",
        "\n",
        "    # Initialize lemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Apply lemmatization and lowercase, filter out single letters and common stopwords\n",
        "    tokens = [lemmatizer.lemmatize(word.lower()) for word in words if len(word) > 1 and word.lower() not in stopwords.words('english')]\n",
        "\n",
        "    # Filter out single characters and symbols\n",
        "    tokens = [word for word in words if len(word) > 1 or re.match(r'[$€%]', word)]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Function to convert numbers to text representations\n",
        "def convert_numbers_to_text(tokens):\n",
        "    converted_tokens = []\n",
        "    for token in tokens:\n",
        "        if token.replace('.', '', 1).isdigit():  # Check if it's a number\n",
        "            converted_token = num2words(token)  # Convert number to text (e.g., \"$1.50\" to \"one point five zero dollars\")\n",
        "            converted_tokens.append(converted_token)\n",
        "        else:\n",
        "            converted_tokens.append(token)\n",
        "    return converted_tokens\n",
        "\n",
        "# Data Preprocessing\n",
        "def preprocess_text(text):\n",
        "\n",
        "    # Define a regular expression pattern to remove URLs and punctuation marks\n",
        "    # pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+|[.,?!:;\"`(){}\\[\\]<>]'\n",
        "    # Remove URLs and punctuation marks\n",
        "    # cleaned_text = re.sub(pattern, '', text)\n",
        "\n",
        "    # Tokenization using custom tokenization function\n",
        "    # tokens = custom_tokenize(cleaned_text)\n",
        "    tokens = word_tokenize(text)\n",
        "    # tokens = sp.encode_as_pieces(text) # Tokenizing Using SentencePiece\n",
        "    # print(\"Tokens:\", tokens)\n",
        "\n",
        "    # Store the count of tokens before removing stopwords\n",
        "    # tokens_before_removal = tokens.copy()\n",
        "\n",
        "    # Remove stopwords\n",
        "    # stop_words = set(stopwords.words('english'))\n",
        "    # removed_words = [word for word in tokens_before_removal if word in stop_words]\n",
        "    # tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Calculate how many stopwords were removed\n",
        "    # removed_stopwords_count = len(tokens_before_removal) - len(tokens)\n",
        "    # print(f\"Removed {removed_stopwords_count} stopwords.\")\n",
        "    # print(f\"Removed words: {removed_words}\")\n",
        "\n",
        "    # stemmer = PorterStemmer()\n",
        "    # words = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    # Numerical handling (convert numbers to text representations)\n",
        "    tokens = convert_numbers_to_text(tokens)\n",
        "\n",
        "    # Join the words back to form preprocessed text\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "\n",
        "    # Print the tokens for debugging\n",
        "    # first_100_tokens = tokens[:100]\n",
        "    # print(first_100_tokens)\n",
        "\n",
        "    # Return the text and sentiment scores\n",
        "    return preprocessed_text\n",
        "\n",
        "# df['Sentence'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ELJfUIOvVQ0Q"
      },
      "outputs": [],
      "source": [
        "# Methods\n",
        "\n",
        "# Function to convert bytes to MB or GB\n",
        "def bytes_to_mb_or_gb(byte_value):\n",
        "    if byte_value >= 1024**3:  # GB\n",
        "        return f\"{byte_value / (1024**3):.2f} GB\"\n",
        "    elif byte_value >= 1024**2:  # MB\n",
        "        return f\"{byte_value / (1024**2):.2f} MB\"\n",
        "    else:  # bytes\n",
        "        return f\"{byte_value} bytes\"\n",
        "\n",
        "def train_and_evaluate_blended_classifier(classifier_combo, additional_info, num_classifiers):\n",
        "    print(f\"Blending the following classifiers: {classifier_combo}\")\n",
        "\n",
        "    start_time = time.time()  # Record the start time\n",
        "    start_time = time.time()\n",
        "    # Get CPU information\n",
        "    num_cores = multiprocessing.cpu_count()\n",
        "    processor_type = platform.processor()\n",
        "    # Get OS information\n",
        "    os_name = platform.system()+'('+platform.release()+')'\n",
        "    # Get RAM information\n",
        "    ram_total = os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES')\n",
        "    ram_total_mb = ram_total / (1024**2)\n",
        "    # Calculate CPU and RAM resources before training\n",
        "    cpu_before = psutil.cpu_percent()\n",
        "    available_memory_before = psutil.virtual_memory().available\n",
        "\n",
        "################################################################################\n",
        "    # Create the voting classifier with the selected combination\n",
        "    selected_classifiers = [(name, classifiers[name]) for name in classifier_combo]\n",
        "    voting_classifier = VotingClassifier(estimators=selected_classifiers, voting='hard')\n",
        "\n",
        "    # Train the voting classifier\n",
        "    voting_classifier.fit(X_train_tfidf_dense, y_train)\n",
        "\n",
        "    # Make final predictions on the test data using the voting classifier\n",
        "    final_predictions = voting_classifier.predict(X_test_tfidf_dense)\n",
        "\n",
        "    # Calculate evaluation metrics for the blended model\n",
        "    accuracy = accuracy_score(y_test, final_predictions)\n",
        "    precision = precision_score(y_test, final_predictions, average='weighted', zero_division=1)\n",
        "    recall = recall_score(y_test, final_predictions, average='weighted')\n",
        "    f1 = f1_score(y_test, final_predictions, average='weighted')\n",
        "    cm = confusion_matrix(y_test, final_predictions)\n",
        "\n",
        "    # Calculate Cohen's kappa\n",
        "    kappa = cohen_kappa_score(y_test, final_predictions)\n",
        "\n",
        "################################################################################\n",
        "\n",
        "    end_time = time.time()  # Record the end time\n",
        "    execution_time = end_time - start_time  # Calculate the execution time\n",
        "    # Calculate CPU and RAM resources after training\n",
        "    cpu_after = psutil.cpu_percent()\n",
        "    available_memory_after = psutil.virtual_memory().available\n",
        "\n",
        "    # Calculate resource usage during this iteration\n",
        "    cpu_usage = max(cpu_after - cpu_before, 0)\n",
        "    memory_usage = max(available_memory_after - available_memory_before, 0)  # Ensure non-negative value\n",
        "    memory_usage_str = bytes_to_mb_or_gb(memory_usage)\n",
        "\n",
        "    print(f\"CPU Usage: {cpu_usage:.2f}%\")\n",
        "    print(f\"Memory Usage: {memory_usage_str}\")\n",
        "\n",
        "    # Store the results in the list as a dictionary\n",
        "    results_dict = {\n",
        "        'Blended Classifiers': classifier_combo,\n",
        "        'Accuracy': accuracy,\n",
        "        'Kappa': kappa,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-Score': f1,\n",
        "        'Confusion Matrix': cm,\n",
        "        'Execution Time (s)': execution_time,\n",
        "        'Total Classifiers': additional_info['Total Classifiers'],\n",
        "        'Total Features': additional_info['Total Features'],\n",
        "        'Training Data Size': additional_info['Training Data Size'],\n",
        "        'Test Data Size': additional_info['Test Data Size'],\n",
        "        'Random State': additional_info['Random State'],\n",
        "        'Preprocessing': additional_info['Preprocessing'],\n",
        "        'SMOTE': additional_info['SMOTE'],\n",
        "        'Total CPU Cores': num_cores,\n",
        "        'CPU Usage (%)': cpu_usage,\n",
        "        'Total RAM': ram_total_mb,\n",
        "        'Memory Usage': memory_usage_str,\n",
        "        'Processor Type': processor_type,\n",
        "        'OS': os_name\n",
        "    }\n",
        "    print(results_dict)\n",
        "\n",
        "\n",
        "    return results_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec:\n",
        "We can use Word2Vec in our code to learn word embeddings from your text data. To do this, we'll need to use a library like Gensim in Python, which provides an easy way to train Word2Vec models."
      ],
      "metadata": {
        "id": "HuE9L_Zpn2MP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-f2rfrDTso-i",
        "outputId": "2ffb20de-a11c-4abf-c043-b8e1546f0a30"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Sentence', 'Sentiment', 'Preprocessed_Sentence'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating our own word2vec model\n",
        "\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "df['Preprocessed_Sentence'] = df['Sentence'].apply(preprocess_text)\n",
        "# print(df.columns)\n",
        "corpus = df['Preprocessed_Sentence'].apply(lambda x: x.split())  # Assuming each sentence is a space-separated list of words\n",
        "\n",
        "# Train Word2Vec model with more epochs\n",
        "word2vec_model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1, sg=0, epochs=10)\n",
        "\n",
        "# Monitor Loss: You can track the loss (negative log likelihood) during training using the get_latest_training_loss() method of the Word2Vec model. This method returns the loss for the most recent batch of training data:\n",
        "# Check the latest training loss\n",
        "latest_loss = word2vec_model.get_latest_training_loss()\n",
        "print(f\"Latest Training Loss: {latest_loss}\")\n",
        "# The loss should decrease over time during training. If it remains constant or increases significantly, it may indicate an issue with your data or training parameters.\n",
        "\n",
        "# Get the list of words in your Word2Vec model's vocabulary\n",
        "vocabulary_words = word2vec_model.wv.index_to_key\n",
        "# Print the first 10 words as an example\n",
        "print(\"First 10 words in the vocabulary:\")\n",
        "print(vocabulary_words[:10])\n",
        "\n",
        "# Test Word Similarity: After training, you can test whether the model has learned meaningful word embeddings by checking word similarity. Gensim's wv attribute provides access to the word vectors. You can use the similarity method to check the similarity between words:\n",
        "# Check word similarity between two specific words\n",
        "# word1 = \"the\"\n",
        "# word2 = \"and\"\n",
        "# similarity_score = word2vec_model.wv.similarity(word1, word2)\n",
        "# print(f\"Similarity between '{word1}' and '{word2}': {similarity_score}\")\n",
        "# Replace \"word1\" and \"word2\" with words from your vocabulary to check their similarity. Higher similarity scores indicate that the model has learned meaningful word representations.\n",
        "\n",
        "# Save the model to a file\n",
        "word2vec_model.save(\"word2vec_model.bin\")\n",
        "\n",
        "# Load the model from the file\n",
        "loaded_model = Word2Vec.load(\"word2vec_model.bin\")\n",
        "\n",
        "loaded_model\n",
        "\n",
        "# Save the trained model to a local file\n",
        "# word2vec_model.save('word2vec_model.bin')\n",
        "\n",
        "# import shutil\n",
        "\n",
        "# # Specify the destination folder in your Google Drive\n",
        "# destination_folder = '/content/Drive/MyDrive/python/financial_sentiment_analysis/'\n",
        "\n",
        "# # Move the trained model to Google Drive\n",
        "# shutil.move('word2vec_model.bin', destination_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYtxGybWn4em",
        "outputId": "f096da87-9917-48e1-acf8-1c9190f41e82"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latest Training Loss: 0.0\n",
            "First 10 words in the vocabulary:\n",
            "['.', 'the', ',', 'and', 'of', 'in', 'to', 'point', 'two', 'a']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gensim.models.word2vec.Word2Vec at 0x7c61dffe7850>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fU_reZlscOfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb5e41dd-7cd8-4fed-b3f9-a9ebf1782978"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of features added by sentiment scores: 7\n",
            "Number of TF-IDF features: 11092\n",
            "Number of features after adding sentiment scores and Word2Vec embeddings: 11199\n"
          ]
        }
      ],
      "source": [
        "# Vectorizing, Balancing, Extracting Features and Splitting the Data.\n",
        "\n",
        "# Constants\n",
        "PREPROCESSING_FLAG = 1\n",
        "SMOTE_FLAG = 1\n",
        "# MAX_FEATURES_LIMIT = 15000\n",
        "TEST_SIZE = 0.2\n",
        "RAND_STATE = 42\n",
        "\n",
        "# Apply data preprocessing to the 'Sentence' column and create 'Preprocessed_Sentence' column\n",
        "df['Preprocessed_Sentence'] = df['Sentence'].apply(preprocess_text)\n",
        "\n",
        "# Prepare the data\n",
        "if PREPROCESSING_FLAG == 1:\n",
        "    X = df['Preprocessed_Sentence']\n",
        "else:\n",
        "    X = df['Sentence']\n",
        "\n",
        "y = df['Sentiment']\n",
        "\n",
        "##################### Sentiment Score - Starts #####################\n",
        "# Initialize lists to store sentiment scores for each document\n",
        "positive_scores = []\n",
        "negative_scores = []\n",
        "uncertainty_scores = []\n",
        "litigious_scores = []\n",
        "strong_modal_scores = []\n",
        "weak_modal_scores = []\n",
        "constraining_scores = []\n",
        "\n",
        "# Iterate through your dataset (replace df with your actual DataFrame)\n",
        "for text in df['Preprocessed_Sentence']:\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Initialize sentiment scores for each document\n",
        "    sentiment_scores = {\n",
        "        'positive': 0,\n",
        "        'negative': 0,\n",
        "        'uncertainty': 0,\n",
        "        'litigious': 0,\n",
        "        'strong_modal': 0,\n",
        "        'weak_modal': 0,\n",
        "        'constraining': 0\n",
        "    }\n",
        "\n",
        "    # Calculate sentiment scores for each token in the document\n",
        "    for token in tokens:\n",
        "        token = token.upper()  # Convert token to uppercase for matching\n",
        "        if token in master_dictionary:\n",
        "            entry = master_dictionary[token]\n",
        "            for sentiment_category in sentiment_scores.keys():\n",
        "                if getattr(entry, sentiment_category) > 0:\n",
        "                    sentiment_scores[sentiment_category] += 1\n",
        "\n",
        "    # Append sentiment scores to their respective lists\n",
        "    positive_scores.append(sentiment_scores['positive'])\n",
        "    negative_scores.append(sentiment_scores['negative'])\n",
        "    uncertainty_scores.append(sentiment_scores['uncertainty'])\n",
        "    litigious_scores.append(sentiment_scores['litigious'])\n",
        "    strong_modal_scores.append(sentiment_scores['strong_modal'])\n",
        "    weak_modal_scores.append(sentiment_scores['weak_modal'])\n",
        "    constraining_scores.append(sentiment_scores['constraining'])\n",
        "\n",
        "# Add sentiment score columns to your DataFrame\n",
        "df['Positive_Score'] = positive_scores\n",
        "df['Negative_Score'] = negative_scores\n",
        "df['Uncertainty_Score'] = uncertainty_scores\n",
        "df['Litigious_Score'] = litigious_scores\n",
        "df['Strong_Modal_Score'] = strong_modal_scores\n",
        "df['Weak_Modal_Score'] = weak_modal_scores\n",
        "df['Constraining_Score'] = constraining_scores\n",
        "\n",
        "# Print positive and negative scores for the first few documents (adjust the range as needed)\n",
        "# for i in range(50):  # Print scores for the first 50 documents as an example\n",
        "#     print(f\"Document {i + 1} - Positive Score: {positive_scores[i]}, Negative Score: {negative_scores[i]}\")\n",
        "# Calculate the number of features added by sentiment scores\n",
        "num_sentiment_score_features = len(['Positive_Score', 'Negative_Score', 'Uncertainty_Score', 'Litigious_Score', 'Strong_Modal_Score', 'Weak_Modal_Score', 'Constraining_Score'])\n",
        "print(f\"Number of features added by sentiment scores: {num_sentiment_score_features}\")\n",
        "\n",
        "######################### Sentiment Scores - Ended ############################\n",
        "\n",
        "######################### TF-IDF - Starts #####################################\n",
        "# Create a TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer() # max_features=MAX_FEATURES_LIMIT\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X)\n",
        "\n",
        "# Convert the labels to numerical values using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Print the number of features before adding sentiment scores\n",
        "print(f\"Number of TF-IDF features: {X_train_tfidf.shape[1]}\")\n",
        "\n",
        "############################## Word2Vec ################################\n",
        "# Load your Word2Vec model (replace \"word2vec_model.bin\" with your actual file)\n",
        "loaded_model = Word2Vec.load(\"word2vec_model.bin\")\n",
        "\n",
        "# Iterate through the documents to create combined feature vectors\n",
        "combined_feature_vectors = []\n",
        "for i, doc in enumerate(df['Preprocessed_Sentence']):\n",
        "    # Tokenize the document\n",
        "    tokens = doc.split()\n",
        "\n",
        "    # Calculate the average Word2Vec embedding for the document\n",
        "    word2vec_embeddings = []\n",
        "    for token in tokens:\n",
        "        if token in loaded_model.wv:\n",
        "            word2vec_embeddings.append(loaded_model.wv[token])\n",
        "\n",
        "    # Calculate the average embedding (you can use other aggregation methods too)\n",
        "    avg_embedding = sum(word2vec_embeddings) / len(word2vec_embeddings) if word2vec_embeddings else [0.0] * loaded_model.vector_size\n",
        "\n",
        "    # Convert the average Word2Vec embedding to a dense array\n",
        "    avg_embedding_dense = [float(x) for x in avg_embedding]\n",
        "\n",
        "    # Combine the TF-IDF features and Word2Vec embeddings for this document\n",
        "    combined_feature_vector = list(X_train_tfidf[i].toarray()[0]) + avg_embedding_dense + [positive_scores[i], negative_scores[i], uncertainty_scores[i], litigious_scores[i], strong_modal_scores[i], weak_modal_scores[i], constraining_scores[i]]\n",
        "\n",
        "    # Append the combined feature vector to the list\n",
        "    combined_feature_vectors.append(combined_feature_vector)\n",
        "\n",
        "# Now you have a list of combined feature vectors with TF-IDF, Word2Vec, and Loughran-McDonald features\n",
        "print(f\"Number of features after adding sentiment scores and Word2Vec embeddings: {len(combined_feature_vectors[0])}\")\n",
        "\n",
        "# Balancing the Dataset\n",
        "smote = SMOTE(sampling_strategy='auto', random_state=RAND_STATE) # random_state=RAND_STATE\n",
        "X_train_balanced, y_train_balanced = smote.fit_resample(combined_feature_vectors, y_encoded)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train_balanced, y_train_balanced, test_size=TEST_SIZE, random_state=RAND_STATE) # random_state=RAND_STATE\n",
        "\n",
        "# Convert X_train_tfidf to a dense numpy array\n",
        "X_train_tfidf_dense = np.array(X_train)\n",
        "\n",
        "# Convert X_test_tfidf to a dense numpy array\n",
        "X_test_tfidf_dense = np.array(X_test)\n",
        "\n",
        "# X_train_tfidf_dense\n",
        "# y_encoded\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hBcE7dNO5nt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "T9NgL91lVsL7"
      },
      "outputs": [],
      "source": [
        "individual_mode = False\n",
        "\n",
        "# Initialize classifiers as base models\n",
        "classifiers = {\n",
        "    # 'Logistic Regression': LogisticRegression(max_iter=1000),\n",
        "    # 'SDG': SGDClassifier(),\n",
        "    # 'Random Forest': RandomForestClassifier(max_depth=10),\n",
        "    # 'Support Vector Machine': SVC(),\n",
        "    'Naive Bayes': MultinomialNB(),\n",
        "    #  'Decision Tree': DecisionTreeClassifier(),\n",
        "    #  'Bagging': BaggingClassifier(),\n",
        "    'Gaussian Naive Bayes': GaussianNB(),\n",
        "    # 'Extreme Gradient Boosting (XGBoost)': XGBClassifier()\n",
        "\n",
        "}\n",
        "\n",
        "# Initialize blended_results_list\n",
        "results_list = []\n",
        "# Initialize blended_results_dict\n",
        "results_dict = {}\n",
        "# Initialize an empty list to store the rows of the results\n",
        "results_rows = []\n",
        "# Additional Information\n",
        "# Additional information about the code\n",
        "additional_info = {\n",
        "    'Total Classifiers': len(classifiers),\n",
        "    'Total Features': len(tfidf_vectorizer.get_feature_names_out()),\n",
        "    'Training Data Size': len(y_train),\n",
        "    'Test Data Size': len(X_test_tfidf_dense),\n",
        "    'Random State': RAND_STATE,  # Replace with the actual random_state value used in train_test_split\n",
        "    'Preprocessing': PREPROCESSING_FLAG,\n",
        "    'SMOTE': SMOTE_FLAG\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "Cx29jTDPZkQ-",
        "outputId": "8b2d009b-8c4d-488e-a9e0-01919317a391"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blending the following classifiers: ('Naive Bayes', 'Gaussian Naive Bayes')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-cae62c926ef5>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcombo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcombinations_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mnum_classifiers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombo\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Get the number of classifiers in the current combination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate_blended_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madditional_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classifiers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mresults_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-d02d257ff22f>\u001b[0m in \u001b[0;36mtrain_and_evaluate_blended_classifier\u001b[0;34m(classifier_combo, additional_info, num_classifiers)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Train the voting classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mvoting_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tfidf_dense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Make final predictions on the test data using the voting classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_voting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mtransformed_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mle_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_voting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m     79\u001b[0m             )\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n\u001b[0m\u001b[1;32m     82\u001b[0m             delayed(_fit_single_estimator)(\n\u001b[1;32m     83\u001b[0m                 \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1793\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py\u001b[0m in \u001b[0;36m_fit_single_estimator\u001b[0;34m(estimator, X, y, sample_weight, message_clsname, message)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_counters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_count\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    896\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0;34m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m         \u001b[0mcheck_non_negative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MultinomialNB (input X)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_non_negative\u001b[0;34m(X, whom)\u001b[0m\n\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mX_min\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Negative values in data passed to %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mwhom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Negative values in data passed to MultinomialNB (input X)"
          ]
        }
      ],
      "source": [
        "# Evaluating\n",
        "\n",
        "for r in range(2, len(classifiers) + 1):\n",
        "    combinations_list = list(combinations(classifiers, r))\n",
        "\n",
        "    # Iterate through each combination and call the train_and_evaluate_blended_classifier method\n",
        "    for combo in combinations_list:\n",
        "        num_classifiers = len(combo)  # Get the number of classifiers in the current combination\n",
        "        results = train_and_evaluate_blended_classifier(combo, additional_info, num_classifiers)\n",
        "        results_list.append(results)\n",
        "\n",
        "# Sort the blended results based on accuracy in descending order\n",
        "sorted_results = sorted(results_list, key=lambda x: (x['Accuracy'], x['Precision'], x['Recall'], x['F1-Score']), reverse=True)\n",
        "\n",
        "# Save the sorted_results and additional information to a DataFrame\n",
        "sorted_results_df = pd.DataFrame(sorted_results)\n",
        "print(sorted_results_df)\n",
        "\n",
        "# Set the time zone to Pakistan Standard Time (PST)\n",
        "pakistan_tz = pytz.timezone('Asia/Karachi')\n",
        "\n",
        "# Get the current date in Pakistan's time zone\n",
        "current_date = datetime.now(pakistan_tz).strftime('%Y-%m-%d')\n",
        "\n",
        "# Append the new results to the existing file or create a new file if it doesn't exist\n",
        "file_name = f\"lm_{current_date}_blended_classifier_results.csv\"\n",
        "file_path = f\"/content/Drive/MyDrive/python/results/blended_classifiers/{file_name}\"\n",
        "\n",
        "try:\n",
        "    if os.path.exists(file_path):\n",
        "        existing_results_df = pd.read_csv(file_path)\n",
        "        final_df = pd.concat([existing_results_df, sorted_results_df], ignore_index=True)\n",
        "    else:\n",
        "        final_df = sorted_results_df\n",
        "\n",
        "    final_df.to_csv(file_path, index=False)\n",
        "    print(f\"Results have been appended to {file_path} in Google Drive.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while saving the file: {str(e)}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMcsO2FirDNleMq3LtQaZQA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}