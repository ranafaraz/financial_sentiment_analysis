{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtK82S9batdu",
        "outputId": "6e8e4e42-5b81-44b7-c7d3-aeba85c8eaba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/Drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/Drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fe9WUxpbbpQK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f3f629d-151b-460f-e6aa-4db3dbcb88bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting num2words\n",
            "  Downloading num2words-0.5.12-py3-none-any.whl (125 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/125.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m122.9/125.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6.2 (from num2words)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=284b33043058721bad000ad94c5a3447db6573d291fd0a8ed1022b99d99774cb\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: docopt, num2words\n",
            "Successfully installed docopt-0.6.2 num2words-0.5.12\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import psutil\n",
        "import platform\n",
        "import scipy.sparse as sp\n",
        "from scipy.sparse import csr_matrix, hstack\n",
        "# import joblib\n",
        "# from itertools import permutations\n",
        "from itertools import combinations, chain\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import pytz # timezone\n",
        "from datetime import datetime\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sys\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from tqdm import tqdm\n",
        "import multiprocessing\n",
        "from multiprocessing import Process, Queue\n",
        "\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, cohen_kappa_score\n",
        "# from sklearn.base import clone\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "# from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "!pip install num2words\n",
        "from num2words import num2words\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYLFMIwAVZk4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/Drive/MyDrive/python/data.csv\")\n",
        "# custom_stopwords_df = pd.read_csv(\"/content/Drive/MyDrive/python/financial_sentiment_analysis/custom_stopwords.csv\")  # StopWord List\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Python module that will load the Loughran-McDonald Master and its components (and optionally separate sentiment dictionaries).\n",
        "\n",
        "But this technique doesn't worked well in our case. It only added few new features."
      ],
      "metadata": {
        "id": "xGX-KhJA3R08"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAZVL9xBgOJO"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "Routine to load master dicitonary\n",
        "Version for LM 2020 Master Dictionary\n",
        "\n",
        "Bill McDonald\n",
        "Date: 201510 Updated: 202201\n",
        "\"\"\"\n",
        "\n",
        "import datetime as dt\n",
        "import sys\n",
        "\n",
        "\n",
        "def load_masterdictionary(file_path, print_flag=False, f_log=None, get_other=False):\n",
        "    start_local = dt.datetime.now()\n",
        "    # Setup dictionaries\n",
        "    _master_dictionary = {}\n",
        "    _sentiment_categories = ['negative', 'positive', 'uncertainty', 'litigious',\n",
        "                             'strong_modal', 'weak_modal', 'constraining']\n",
        "    _sentiment_dictionaries = dict()\n",
        "    for sentiment in _sentiment_categories:\n",
        "        _sentiment_dictionaries[sentiment] = dict()\n",
        "\n",
        "    # Load slightly modified common stopwords.\n",
        "    # Dropped from traditional: A, I, S, T, DON, WILL, AGAINST\n",
        "    # Added: AMONG\n",
        "    _stopwords = ['ME', 'MY', 'MYSELF', 'WE', 'OUR', 'OURS', 'OURSELVES', 'YOU', 'YOUR', 'YOURS',\n",
        "                  'YOURSELF', 'YOURSELVES', 'HE', 'HIM', 'HIS', 'HIMSELF', 'SHE', 'HER', 'HERS', 'HERSELF',\n",
        "                  'IT', 'ITS', 'ITSELF', 'THEY', 'THEM', 'THEIR', 'THEIRS', 'THEMSELVES', 'WHAT', 'WHICH',\n",
        "                  'WHO', 'WHOM', 'THIS', 'THAT', 'THESE', 'THOSE', 'AM', 'IS', 'ARE', 'WAS', 'WERE', 'BE',\n",
        "                  'BEEN', 'BEING', 'HAVE', 'HAS', 'HAD', 'HAVING', 'DO', 'DOES', 'DID', 'DOING', 'AN',\n",
        "                  'THE', 'AND', 'BUT', 'IF', 'OR', 'BECAUSE', 'AS', 'UNTIL', 'WHILE', 'OF', 'AT', 'BY',\n",
        "                  'FOR', 'WITH', 'ABOUT', 'BETWEEN', 'INTO', 'THROUGH', 'DURING', 'BEFORE',\n",
        "                  'AFTER', 'ABOVE', 'BELOW', 'TO', 'FROM', 'UP', 'DOWN', 'IN', 'OUT', 'ON', 'OFF', 'OVER',\n",
        "                  'UNDER', 'AGAIN', 'FURTHER', 'THEN', 'ONCE', 'HERE', 'THERE', 'WHEN', 'WHERE', 'WHY',\n",
        "                  'HOW', 'ALL', 'ANY', 'BOTH', 'EACH', 'FEW', 'MORE', 'MOST', 'OTHER', 'SOME', 'SUCH',\n",
        "                  'NO', 'NOR', 'NOT', 'ONLY', 'OWN', 'SAME', 'SO', 'THAN', 'TOO', 'VERY', 'CAN',\n",
        "                  'JUST', 'SHOULD', 'NOW', 'AMONG']\n",
        "\n",
        "    # Loop thru words and load dictionaries\n",
        "    with open(file_path) as f:\n",
        "        _total_documents = 0\n",
        "        _md_header = f.readline()  # Consume header line\n",
        "        print()\n",
        "        for line in f:\n",
        "            cols = line.rstrip('\\n').split(',')\n",
        "            word = cols[0]\n",
        "            _master_dictionary[word] = MasterDictionary(cols, _stopwords)\n",
        "            for sentiment in _sentiment_categories:\n",
        "                if getattr(_master_dictionary[word], sentiment):\n",
        "                    _sentiment_dictionaries[sentiment][word] = 0\n",
        "            _total_documents += _master_dictionary[cols[0]].doc_count\n",
        "            if len(_master_dictionary) % 5000 == 0 and print_flag:\n",
        "                print(f'\\r ...Loading Master Dictionary {len(_master_dictionary):,}', end='', flush=True)\n",
        "\n",
        "    if print_flag:\n",
        "        print('\\r', end='')  # clear line\n",
        "        print(f'\\nMaster Dictionary loaded from file:\\n  {file_path}\\n')\n",
        "        print(f'  master_dictionary has {len(_master_dictionary):,} words.\\n')\n",
        "\n",
        "    if f_log:\n",
        "        try:\n",
        "            f_log.write('\\n\\n  FUNCTION: load_masterdictionary' +\n",
        "                        '(file_path, print_flag, f_log, get_other)\\n')\n",
        "            f_log.write(f'\\n    file_path  = {file_path}')\n",
        "            f_log.write(f'\\n    print_flag = {print_flag}')\n",
        "            f_log.write(f'\\n    f_log      = {f_log.name}')\n",
        "            f_log.write(f'\\n    get_other  = {get_other}')\n",
        "            f_log.write(f'\\n\\n    {len(_master_dictionary):,} words loaded in master_dictionary.\\n')\n",
        "            f_log.write(f'\\n    Sentiment:')\n",
        "            for sentiment in _sentiment_categories:\n",
        "                f_log.write(f'\\n      {sentiment:13}: {len(_sentiment_dictionaries[sentiment]):8,}')\n",
        "            f_log.write(f'\\n\\n  END FUNCTION: load_masterdictionary: {(dt.datetime.now()-start_local)}')\n",
        "        except Exception as e:\n",
        "            print('Log file in load_masterdictionary is not available for writing')\n",
        "            print(f'Error = {e}')\n",
        "\n",
        "    if get_other:\n",
        "        return _master_dictionary, _md_header, _sentiment_categories, _sentiment_dictionaries, _stopwords, _total_documents\n",
        "    else:\n",
        "        return _master_dictionary\n",
        "\n",
        "\n",
        "class MasterDictionary:\n",
        "    def __init__(self, cols, _stopwords):\n",
        "        for ptr, col in enumerate(cols):\n",
        "            if col == '':\n",
        "                cols[ptr] = '0'\n",
        "        try:\n",
        "            self.word = cols[0].upper()\n",
        "            self.sequence_number = int(cols[1])\n",
        "            self.word_count = int(cols[2])\n",
        "            self.word_proportion = float(cols[3])\n",
        "            self.average_proportion = float(cols[4])\n",
        "            self.std_dev_prop = float(cols[5])\n",
        "            self.doc_count = int(cols[6])\n",
        "            self.negative = int(cols[7])\n",
        "            self.positive = int(cols[8])\n",
        "            self.uncertainty = int(cols[9])\n",
        "            self.litigious = int(cols[10])\n",
        "            self.strong_modal = int(cols[11])\n",
        "            self.weak_modal = int(cols[12])\n",
        "            self.constraining = int(cols[13])\n",
        "            self.syllables = int(cols[14])\n",
        "            self.source = cols[15]\n",
        "            if self.word in _stopwords:\n",
        "                self.stopword = True\n",
        "            else:\n",
        "                self.stopword = False\n",
        "        except:\n",
        "            print('ERROR in class MasterDictionary')\n",
        "            print(f'word = {cols[0]} : seqnum = {cols[1]}')\n",
        "            quit()\n",
        "        return\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calling the method to load Loughran-McDonald Master Dictionary\n",
        "if __name__ == '__main__':\n",
        "    start = dt.datetime.now()\n",
        "    print(f'\\n\\n{start.strftime(\"%c\")}\\nPROGRAM NAME: {sys.argv[0]}\\n')\n",
        "    f_log = open('/content/Drive/MyDrive/python/financial_sentiment_analysis/Load_MD_Logfile.txt', 'w')\n",
        "    md = (r\"/content/Drive/MyDrive/python/financial_sentiment_analysis/Loughran-McDonald_MasterDictionary_1993-2021.csv\")\n",
        "    master_dictionary, md_header, sentiment_categories, sentiment_dictionaries, stopwords, total_documents = \\\n",
        "        load_masterdictionary(md, True, f_log, True)\n",
        "    print(f'\\n\\nRuntime: {(dt.datetime.now()-start)}')\n",
        "    print(f'\\nNormal termination.\\n{dt.datetime.now().strftime(\"%c\")}\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MW9ZzBB9QEHt",
        "outputId": "64203948-d5d0-4a5b-9b62-43e8be91bdee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Sat Sep 30 17:07:15 2023\n",
            "PROGRAM NAME: /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\n",
            "\n",
            "\n",
            " ...Loading Master Dictionary 85,000\n",
            "Master Dictionary loaded from file:\n",
            "  /content/Drive/MyDrive/python/financial_sentiment_analysis/Loughran-McDonald_MasterDictionary_1993-2021.csv\n",
            "\n",
            "  master_dictionary has 86,531 words.\n",
            "\n",
            "\n",
            "\n",
            "Runtime: 0:00:03.038235\n",
            "\n",
            "Normal termination.\n",
            "Sat Sep 30 17:07:18 2023\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subword tokenization using libraries like SentencePiece or Byte Pair Encoding (BPE) is useful when you want to tokenize text into subword units. This can be particularly helpful for languages with complex morphology or when dealing with out-of-vocabulary words. Here's how you can use these libraries:\n",
        "\n",
        "Using SentencePiece:\n",
        "SentencePiece is a popular subword tokenization library developed by Google. It's used for a wide range of natural language processing tasks.\n",
        "\n",
        "But this technique doesn't worked in our case. Accuracy and number of features fell dramatically, as soon as we applied it."
      ],
      "metadata": {
        "id": "HeoYkeAH3rLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train a SentencePiece model for Tokenization\n",
        "!pip install sentencepiece\n",
        "import sentencepiece as spm\n",
        "spm.SentencePieceTrainer.train(input=(r\"/content/Drive/MyDrive/python/data.csv\"), model_prefix='mymodel', vocab_size=10000) # vocab_size=5000\n",
        "sp = spm.SentencePieceProcessor(model_file='mymodel.model')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqe6zVVwPeM8",
        "outputId": "ba24aadc-5288-42d5-803a-bc7031eec6c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "A8NsKSDV4q12"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2wklQSHh4xQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell is dedicated for preprocessing and tokenization processes."
      ],
      "metadata": {
        "id": "LMb_h3O3fu0K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oqnzngiVoA4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63e5fad3-3c20-408b-bf16-f87fee452130"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    geosolutions technology leverage benefon 's gp...\n",
            "1    esi low one euro, fifty cents two euro, fifty ...\n",
            "2    last quarter two thousand and ten euro, zero c...\n",
            "3    according finnish-russian chamber commerce maj...\n",
            "4    swedish buyout firm sold remaining twenty-two ...\n",
            "Name: Preprocessed_Sentence, dtype: object\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Custom financial domain-specific stopwords\n",
        "custom_stopwords = set([\"stock\", \"price\", \"earnings\", \"report\", \"investors\", \"company\"])\n",
        "\n",
        "# Function to convert numbers to text representations\n",
        "def convert_numbers_to_text(tokens):\n",
        "    converted_tokens = []\n",
        "    for token in tokens:\n",
        "        # Check if it's a number (integer or decimal)\n",
        "        if re.match(r'^-?\\d+(?:\\.\\d+)?$', token):\n",
        "            converted_token = num2words(token, to='currency')  # Convert number to text with currency format\n",
        "            converted_tokens.extend(converted_token.split())  # Split the converted text\n",
        "        else:\n",
        "            converted_tokens.append(token)\n",
        "    return converted_tokens\n",
        "\n",
        "# Tokenization and preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove HTML tags and URLs\n",
        "    text = re.sub(r\"<.*?>\", \"\", text)\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
        "\n",
        "    # Tokenize text using NLTK's word_tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Initialize lemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Apply lemmatization, filter out stopwords, and custom financial stopwords\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if len(word) > 1 and word.lower() not in stopwords.words('english') and word.lower() not in custom_stopwords]\n",
        "\n",
        "    # Filter out single characters and symbols\n",
        "    tokens = [word for word in tokens if len(word) > 1] #  or re.match(r'[$€%]', word)\n",
        "\n",
        "    # Convert numbers to text representations (including currency format)\n",
        "    tokens = convert_numbers_to_text(tokens)\n",
        "\n",
        "    # Join the tokens back to form preprocessed text\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "# Preprocess and tokenize the financial text\n",
        "df['Preprocessed_Sentence'] = df['Sentence'].apply(preprocess_text)\n",
        "\n",
        "# Access the 'Preprocessed_Sentence' column\n",
        "preprocessed_sentences = df['Preprocessed_Sentence']\n",
        "\n",
        "# Print the first few preprocessed sentences for verification\n",
        "print(preprocessed_sentences.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELJfUIOvVQ0Q"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Methods\n",
        "\n",
        "# Function to convert bytes to MB or GB\n",
        "def bytes_to_mb_or_gb(byte_value):\n",
        "    if byte_value >= 1024**3:  # GB\n",
        "        return f\"{byte_value / (1024**3):.2f} GB\"\n",
        "    elif byte_value >= 1024**2:  # MB\n",
        "        return f\"{byte_value / (1024**2):.2f} MB\"\n",
        "    else:  # bytes\n",
        "        return f\"{byte_value} bytes\"\n",
        "\n",
        "def train_and_evaluate_blended_classifier(classifier_combo, additional_info, num_classifiers):\n",
        "    print(f\"Blending the following classifiers: {classifier_combo}\")\n",
        "\n",
        "    start_time = time.time()  # Record the start time\n",
        "    start_time = time.time()\n",
        "    # Get CPU information\n",
        "    num_cores = multiprocessing.cpu_count()\n",
        "    processor_type = platform.processor()\n",
        "    # Get OS information\n",
        "    os_name = platform.system()+'('+platform.release()+')'\n",
        "    # Get RAM information\n",
        "    ram_total = os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES')\n",
        "    ram_total_mb = ram_total / (1024**2)\n",
        "    # Calculate CPU and RAM resources before training\n",
        "    cpu_before = psutil.cpu_percent()\n",
        "    available_memory_before = psutil.virtual_memory().available\n",
        "\n",
        "################################################################################\n",
        "    # Create the voting classifier with the selected combination\n",
        "    selected_classifiers = [(name, classifiers[name]) for name in classifier_combo]\n",
        "    voting_classifier = VotingClassifier(estimators=selected_classifiers, voting='hard')\n",
        "\n",
        "    # Train the voting classifier\n",
        "    voting_classifier.fit(X_train_tfidf_dense, y_train)\n",
        "\n",
        "    # Make final predictions on the test data using the voting classifier\n",
        "    final_predictions = voting_classifier.predict(X_test_tfidf_dense)\n",
        "\n",
        "    # Calculate evaluation metrics for the blended model\n",
        "    accuracy = accuracy_score(y_test, final_predictions)\n",
        "    precision = precision_score(y_test, final_predictions, average='weighted', zero_division=1)\n",
        "    recall = recall_score(y_test, final_predictions, average='weighted')\n",
        "    f1 = f1_score(y_test, final_predictions, average='weighted')\n",
        "    cm = confusion_matrix(y_test, final_predictions)\n",
        "\n",
        "    # Calculate Cohen's kappa\n",
        "    kappa = cohen_kappa_score(y_test, final_predictions)\n",
        "\n",
        "################################################################################\n",
        "\n",
        "    end_time = time.time()  # Record the end time\n",
        "    execution_time = end_time - start_time  # Calculate the execution time\n",
        "    # Calculate CPU and RAM resources after training\n",
        "    cpu_after = psutil.cpu_percent()\n",
        "    available_memory_after = psutil.virtual_memory().available\n",
        "\n",
        "    # Calculate resource usage during this iteration\n",
        "    cpu_usage = max(cpu_after - cpu_before, 0)\n",
        "    memory_usage = max(available_memory_after - available_memory_before, 0)  # Ensure non-negative value\n",
        "    memory_usage_str = bytes_to_mb_or_gb(memory_usage)\n",
        "\n",
        "    print(f\"CPU Usage: {cpu_usage:.2f}%\")\n",
        "    print(f\"Memory Usage: {memory_usage_str}\")\n",
        "\n",
        "    # Store the results in the list as a dictionary\n",
        "    results_dict = {\n",
        "        'Blended Classifiers': classifier_combo,\n",
        "        'Accuracy': accuracy,\n",
        "        'Kappa': kappa,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-Score': f1,\n",
        "        'Confusion Matrix': cm,\n",
        "        'Execution Time (s)': execution_time,\n",
        "        'Total Classifiers': additional_info['Total Classifiers'],\n",
        "        'Total Features': additional_info['Total Features'],\n",
        "        'Training Data Size': additional_info['Training Data Size'],\n",
        "        'Test Data Size': additional_info['Test Data Size'],\n",
        "        'Random State': additional_info['Random State'],\n",
        "        'Preprocessing': additional_info['Preprocessing'],\n",
        "        'SMOTE': additional_info['SMOTE'],\n",
        "        'Total CPU Cores': num_cores,\n",
        "        'CPU Usage (%)': cpu_usage,\n",
        "        'Total RAM': ram_total_mb,\n",
        "        'Memory Usage': memory_usage_str,\n",
        "        'Processor Type': processor_type,\n",
        "        'OS': os_name\n",
        "    }\n",
        "    print(results_dict)\n",
        "\n",
        "\n",
        "    return results_dict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec:\n",
        "We can use Word2Vec in our code to learn word embeddings from your text data. To do this, we'll need to use a library like Gensim in Python, which provides an easy way to train Word2Vec models."
      ],
      "metadata": {
        "id": "HuE9L_Zpn2MP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-f2rfrDTso-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generating our own word2vec model\n",
        "\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "df['Preprocessed_Sentence'] = df['Sentence'].apply(preprocess_text)\n",
        "# print(df.columns)\n",
        "corpus = df['Preprocessed_Sentence'].apply(lambda x: x.split())  # Assuming each sentence is a space-separated list of words\n",
        "\n",
        "# Train Word2Vec model with more epochs\n",
        "word2vec_model = Word2Vec(sentences=corpus, vector_size=300, window=5, min_count=1, sg=0, epochs=10)\n",
        "\n",
        "# Monitor Loss: You can track the loss (negative log likelihood) during training using the get_latest_training_loss() method of the Word2Vec model. This method returns the loss for the most recent batch of training data:\n",
        "# Check the latest training loss\n",
        "latest_loss = word2vec_model.get_latest_training_loss()\n",
        "print(f\"Latest Training Loss: {latest_loss}\")\n",
        "# The loss should decrease over time during training. If it remains constant or increases significantly, it may indicate an issue with your data or training parameters.\n",
        "\n",
        "# Get the list of words in your Word2Vec model's vocabulary\n",
        "vocabulary_words = word2vec_model.wv.index_to_key\n",
        "# Print the first 10 words as an example\n",
        "print(\"First 10 words in the vocabulary:\")\n",
        "print(vocabulary_words[:10])\n",
        "\n",
        "# Test Word Similarity: After training, you can test whether the model has learned meaningful word embeddings by checking word similarity. Gensim's wv attribute provides access to the word vectors. You can use the similarity method to check the similarity between words:\n",
        "# Check word similarity between two specific words\n",
        "# word1 = \"the\"\n",
        "# word2 = \"and\"\n",
        "# similarity_score = word2vec_model.wv.similarity(word1, word2)\n",
        "# print(f\"Similarity between '{word1}' and '{word2}': {similarity_score}\")\n",
        "# Replace \"word1\" and \"word2\" with words from your vocabulary to check their similarity. Higher similarity scores indicate that the model has learned meaningful word representations.\n",
        "\n",
        "# Save the model to a file\n",
        "word2vec_model.save(\"word2vec_model.bin\")\n",
        "\n",
        "# Load the model from the file\n",
        "loaded_model = Word2Vec.load(\"word2vec_model.bin\")\n",
        "\n",
        "loaded_model\n",
        "\n",
        "# Save the trained model to a local file\n",
        "# word2vec_model.save('word2vec_model.bin')\n",
        "\n",
        "# import shutil\n",
        "\n",
        "# # Specify the destination folder in your Google Drive\n",
        "# destination_folder = '/content/Drive/MyDrive/python/financial_sentiment_analysis/'\n",
        "\n",
        "# # Move the trained model to Google Drive\n",
        "# shutil.move('word2vec_model.bin', destination_folder)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYtxGybWn4em",
        "outputId": "c88b5cc2-0196-4818-c79c-103943213e7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latest Training Loss: 0.0\n",
            "First 10 words in the vocabulary:\n",
            "['euro,', 'cents', 'zero', 'and', 'two', 'thousand', 'eur', \"'s\", 'mn', 'hundred']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gensim.models.word2vec.Word2Vec at 0x7bc511769e10>"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Features Extracted from TF-IDF"
      ],
      "metadata": {
        "id": "oKvWeFciaE22"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fU_reZlscOfX"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Vectorizing, Balancing, Extracting Features and Splitting the Data.\n",
        "\n",
        "# Constants\n",
        "PREPROCESSING_FLAG = 1\n",
        "SMOTE_FLAG = 1\n",
        "TEST_SIZE = 0.2\n",
        "RAND_STATE = 42\n",
        "\n",
        "# Apply data preprocessing to the 'Sentence' column and create 'Preprocessed_Sentence' column\n",
        "df['Preprocessed_Sentence'] = df['Sentence'].apply(preprocess_text)\n",
        "\n",
        "# Prepare the data\n",
        "if PREPROCESSING_FLAG == 1:\n",
        "    X = df['Preprocessed_Sentence']\n",
        "else:\n",
        "    X = df['Sentence']\n",
        "\n",
        "y = df['Sentiment']\n",
        "\n",
        "# Create a TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X)\n",
        "\n",
        "# Convert the labels to numerical values using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Balancing the Dataset\n",
        "smote = SMOTE(sampling_strategy='auto', random_state=RAND_STATE)\n",
        "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_tfidf, y_encoded)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train_balanced, y_train_balanced, test_size=TEST_SIZE, random_state=RAND_STATE)\n",
        "\n",
        "# Convert X_train_tfidf to a dense numpy array\n",
        "X_train_tfidf_dense = X_train.toarray()\n",
        "\n",
        "# Convert X_test_tfidf to a dense numpy array\n",
        "X_test_tfidf_dense = X_test.toarray()\n",
        "\n",
        "if np.any(X_train_tfidf_dense < 0):\n",
        "    # Handle or debug the presence of negative values in TF-IDF features\n",
        "    print(\"Negative values found in TF-IDF features.\")\n",
        "\n",
        "\n",
        "# X_train_tfidf_dense\n",
        "# y_encoded\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following code, combining features extracted from TF-IDF and Word2Vec. This cell will overwrite variables from the previous cell. Only use when you want to use both techniques. Note: Accuracy falls in this case.\n"
      ],
      "metadata": {
        "id": "_ZMTvcRgavEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Constants\n",
        "PREPROCESSING_FLAG = 1\n",
        "SMOTE_FLAG = 1\n",
        "TEST_SIZE = 0.2\n",
        "RAND_STATE = 42\n",
        "\n",
        "# Apply data preprocessing to the 'Sentence' column and create 'Preprocessed_Sentence' column\n",
        "df['Preprocessed_Sentence'] = df['Sentence'].apply(preprocess_text)\n",
        "\n",
        "# Prepare the data\n",
        "if PREPROCESSING_FLAG == 1:\n",
        "    X = df['Preprocessed_Sentence']\n",
        "else:\n",
        "    X = df['Sentence']\n",
        "\n",
        "y = df['Sentiment']\n",
        "\n",
        "# Create a TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X)\n",
        "\n",
        "# Print the number of features added by TF-IDF\n",
        "print(f\"Number of features added by TF-IDF: {X_train_tfidf.shape[1]}\")\n",
        "\n",
        "# Convert the labels to numerical values using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Create Word2Vec features\n",
        "word2vec_features = []\n",
        "\n",
        "for doc in df['Preprocessed_Sentence']:\n",
        "    tokens = doc.split()\n",
        "    word2vec_embeddings = []\n",
        "\n",
        "    for token in tokens:\n",
        "        if token in loaded_model.wv:\n",
        "            word2vec_embeddings.append(loaded_model.wv[token])\n",
        "\n",
        "    # Calculate the average Word2Vec embedding for the document\n",
        "    avg_embedding = sum(word2vec_embeddings) / len(word2vec_embeddings) if word2vec_embeddings else [0.0] * loaded_model.vector_size\n",
        "\n",
        "    # Convert the average Word2Vec embedding to a dense array\n",
        "    avg_embedding_dense = [float(x) for x in avg_embedding]\n",
        "\n",
        "    word2vec_features.append(avg_embedding_dense)\n",
        "\n",
        "# Convert the list of Word2Vec features to a numpy array\n",
        "X_word2vec = np.array(word2vec_features)\n",
        "\n",
        "# Print the number of features added by Word2Vec\n",
        "print(f\"Number of features added by Word2Vec: {X_word2vec.shape[1]}\")\n",
        "\n",
        "# Concatenate TF-IDF and Word2Vec features\n",
        "X_combined = np.concatenate((X_train_tfidf.toarray(), X_word2vec), axis=1)\n",
        "\n",
        "# Balancing the Dataset (if needed)\n",
        "if SMOTE_FLAG == 1:\n",
        "    smote = SMOTE(sampling_strategy='auto', random_state=RAND_STATE)\n",
        "    X_train_balanced, y_train_balanced = smote.fit_resample(X_combined, y_encoded)\n",
        "else:\n",
        "    X_train_balanced, y_train_balanced = X_combined, y_encoded\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train_tfidf_dense, X_test_tfidf_dense, y_train, y_test = train_test_split(X_train_balanced, y_train_balanced, test_size=TEST_SIZE, random_state=RAND_STATE)\n",
        "\n",
        "# Convert X_train_tfidf to a dense numpy array\n",
        "# X_train_tfidf_dense = X_train.toarray()\n",
        "\n",
        "# Convert X_test_tfidf to a dense numpy array\n",
        "# X_test_tfidf_dense = X_test.toarray()\n",
        "\n",
        "# Total number of features\n",
        "print(f\"Total Number of Features added by TF-IDF + Word2Vec= {X_train_tfidf.shape[1] + X_word2vec.shape[1]}\")\n",
        "\n",
        "if np.any(X_train_tfidf_dense < 0):\n",
        "    # Handle or debug the presence of negative values in TF-IDF features\n",
        "    print(\"Negative values found in TF-IDF features.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBcE7dNO5nt7",
        "outputId": "a526c566-875a-4af3-efc4-211e75f529ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of features added by TF-IDF: 9857\n",
            "Number of features added by Word2Vec: 300\n",
            "Total Number of Features added by TF-IDF + Word2Vec= 10157\n",
            "Negative values found in TF-IDF features.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9NgL91lVsL7"
      },
      "outputs": [],
      "source": [
        "\n",
        "individual_mode = False\n",
        "\n",
        "# Initialize classifiers as base models\n",
        "classifiers = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
        "    'SDG': SGDClassifier(),\n",
        "    # 'Random Forest': RandomForestClassifier(max_depth=10),\n",
        "    'Support Vector Machine': SVC(),\n",
        "    'Naive Bayes': MultinomialNB(),\n",
        "    #  'Decision Tree': DecisionTreeClassifier(),\n",
        "     'Bagging': BaggingClassifier(),\n",
        "    # 'Gaussian Naive Bayes': GaussianNB(),\n",
        "    # 'Extreme Gradient Boosting (XGBoost)': XGBClassifier()\n",
        "\n",
        "}\n",
        "\n",
        "# Initialize blended_results_list\n",
        "results_list = []\n",
        "# Initialize blended_results_dict\n",
        "results_dict = {}\n",
        "# Initialize an empty list to store the rows of the results\n",
        "results_rows = []\n",
        "# Additional Information\n",
        "# Additional information about the code\n",
        "additional_info = {\n",
        "    'Total Classifiers': len(classifiers),\n",
        "    'Total Features': len(tfidf_vectorizer.get_feature_names_out()),\n",
        "    # 'Total Features': X_train_tfidf.shape[1] + X_word2vec.shape[1],\n",
        "    'Training Data Size': len(y_train),\n",
        "    'Test Data Size': len(X_test_tfidf_dense),\n",
        "    'Random State': RAND_STATE,  # Replace with the actual random_state value used in train_test_split\n",
        "    'Preprocessing': PREPROCESSING_FLAG,\n",
        "    'SMOTE': SMOTE_FLAG\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cx29jTDPZkQ-",
        "outputId": "36281dab-e61e-4462-d0b6-c266e914dc9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blending the following classifiers: ('Logistic Regression', 'SDG')\n",
            "CPU Usage: 68.60%\n",
            "Memory Usage: 0 bytes\n",
            "{'Blended Classifiers': ('Logistic Regression', 'SDG'), 'Accuracy': 0.8184238551650692, 'Kappa': 0.7275282428665275, 'Precision': 0.8229748146177056, 'Recall': 0.8184238551650692, 'F1-Score': 0.8159920090342314, 'Confusion Matrix': array([[597,  24,  10],\n",
            "       [133, 443,  56],\n",
            "       [ 39,  79, 497]]), 'Execution Time (s)': 69.96645736694336, 'Total Classifiers': 5, 'Total Features': 9857, 'Training Data Size': 7512, 'Test Data Size': 1878, 'Random State': 42, 'Preprocessing': 1, 'SMOTE': 1, 'Total CPU Cores': 2, 'CPU Usage (%)': 68.60000000000001, 'Total RAM': 12982.625, 'Memory Usage': '0 bytes', 'Processor Type': 'x86_64', 'OS': 'Linux(5.15.120+)'}\n",
            "Blending the following classifiers: ('Logistic Regression', 'Support Vector Machine')\n",
            "CPU Usage: 0.00%\n",
            "Memory Usage: 0 bytes\n",
            "{'Blended Classifiers': ('Logistic Regression', 'Support Vector Machine'), 'Accuracy': 0.821618743343983, 'Kappa': 0.7321542608900007, 'Precision': 0.834726113892284, 'Recall': 0.821618743343983, 'F1-Score': 0.8213497738201873, 'Confusion Matrix': array([[594,  36,   1],\n",
            "       [125, 490,  17],\n",
            "       [ 36, 120, 459]]), 'Execution Time (s)': 536.6130132675171, 'Total Classifiers': 5, 'Total Features': 9857, 'Training Data Size': 7512, 'Test Data Size': 1878, 'Random State': 42, 'Preprocessing': 1, 'SMOTE': 1, 'Total CPU Cores': 2, 'CPU Usage (%)': 0, 'Total RAM': 12982.625, 'Memory Usage': '0 bytes', 'Processor Type': 'x86_64', 'OS': 'Linux(5.15.120+)'}\n",
            "Blending the following classifiers: ('Logistic Regression', 'Naive Bayes')\n",
            "CPU Usage: 0.00%\n",
            "Memory Usage: 0 bytes\n",
            "{'Blended Classifiers': ('Logistic Regression', 'Naive Bayes'), 'Accuracy': 0.7529286474973376, 'Kappa': 0.6289684854502705, 'Precision': 0.7823703788183952, 'Recall': 0.7529286474973376, 'F1-Score': 0.7503254942526446, 'Confusion Matrix': array([[597,  25,   9],\n",
            "       [167, 420,  45],\n",
            "       [147,  71, 397]]), 'Execution Time (s)': 51.804733991622925, 'Total Classifiers': 5, 'Total Features': 9857, 'Training Data Size': 7512, 'Test Data Size': 1878, 'Random State': 42, 'Preprocessing': 1, 'SMOTE': 1, 'Total CPU Cores': 2, 'CPU Usage (%)': 0, 'Total RAM': 12982.625, 'Memory Usage': '0 bytes', 'Processor Type': 'x86_64', 'OS': 'Linux(5.15.120+)'}\n",
            "Blending the following classifiers: ('Logistic Regression', 'Bagging')\n",
            "CPU Usage: 0.00%\n",
            "Memory Usage: 17.08 MB\n",
            "{'Blended Classifiers': ('Logistic Regression', 'Bagging'), 'Accuracy': 0.7816826411075612, 'Kappa': 0.6721492960181286, 'Precision': 0.7965554152159836, 'Recall': 0.7816826411075612, 'F1-Score': 0.7798145903667363, 'Confusion Matrix': array([[586,  37,   8],\n",
            "       [138, 463,  31],\n",
            "       [ 71, 125, 419]]), 'Execution Time (s)': 357.46036887168884, 'Total Classifiers': 5, 'Total Features': 9857, 'Training Data Size': 7512, 'Test Data Size': 1878, 'Random State': 42, 'Preprocessing': 1, 'SMOTE': 1, 'Total CPU Cores': 2, 'CPU Usage (%)': 0, 'Total RAM': 12982.625, 'Memory Usage': '17.08 MB', 'Processor Type': 'x86_64', 'OS': 'Linux(5.15.120+)'}\n",
            "Blending the following classifiers: ('SDG', 'Support Vector Machine')\n",
            "CPU Usage: 67.50%\n",
            "Memory Usage: 0 bytes\n",
            "{'Blended Classifiers': ('SDG', 'Support Vector Machine'), 'Accuracy': 0.8258785942492013, 'Kappa': 0.738556365011086, 'Precision': 0.8398254929609973, 'Recall': 0.8258785942492013, 'F1-Score': 0.8246920224095013, 'Confusion Matrix': array([[612,  18,   1],\n",
            "       [138, 478,  16],\n",
            "       [ 40, 114, 461]]), 'Execution Time (s)': 483.1531615257263, 'Total Classifiers': 5, 'Total Features': 9857, 'Training Data Size': 7512, 'Test Data Size': 1878, 'Random State': 42, 'Preprocessing': 1, 'SMOTE': 1, 'Total CPU Cores': 2, 'CPU Usage (%)': 67.5, 'Total RAM': 12982.625, 'Memory Usage': '0 bytes', 'Processor Type': 'x86_64', 'OS': 'Linux(5.15.120+)'}\n",
            "Blending the following classifiers: ('SDG', 'Naive Bayes')\n",
            "CPU Usage: 64.70%\n",
            "Memory Usage: 0 bytes\n",
            "{'Blended Classifiers': ('SDG', 'Naive Bayes'), 'Accuracy': 0.7577209797657082, 'Kappa': 0.6361484855168795, 'Precision': 0.7937333014902774, 'Recall': 0.7577209797657082, 'F1-Score': 0.7549271241079558, 'Confusion Matrix': array([[611,  17,   3],\n",
            "       [176, 414,  42],\n",
            "       [152,  65, 398]]), 'Execution Time (s)': 17.65996503829956, 'Total Classifiers': 5, 'Total Features': 9857, 'Training Data Size': 7512, 'Test Data Size': 1878, 'Random State': 42, 'Preprocessing': 1, 'SMOTE': 1, 'Total CPU Cores': 2, 'CPU Usage (%)': 64.7, 'Total RAM': 12982.625, 'Memory Usage': '0 bytes', 'Processor Type': 'x86_64', 'OS': 'Linux(5.15.120+)'}\n",
            "Blending the following classifiers: ('SDG', 'Bagging')\n",
            "CPU Usage: 66.80%\n",
            "Memory Usage: 21.84 MB\n",
            "{'Blended Classifiers': ('SDG', 'Bagging'), 'Accuracy': 0.7838125665601704, 'Kappa': 0.675328209052272, 'Precision': 0.8028935812422016, 'Recall': 0.7838125665601704, 'F1-Score': 0.780759499050281, 'Confusion Matrix': array([[609,  21,   1],\n",
            "       [155, 447,  30],\n",
            "       [ 76, 123, 416]]), 'Execution Time (s)': 297.34953260421753, 'Total Classifiers': 5, 'Total Features': 9857, 'Training Data Size': 7512, 'Test Data Size': 1878, 'Random State': 42, 'Preprocessing': 1, 'SMOTE': 1, 'Total CPU Cores': 2, 'CPU Usage (%)': 66.8, 'Total RAM': 12982.625, 'Memory Usage': '21.84 MB', 'Processor Type': 'x86_64', 'OS': 'Linux(5.15.120+)'}\n",
            "Blending the following classifiers: ('Support Vector Machine', 'Naive Bayes')\n",
            "CPU Usage: 65.40%\n",
            "Memory Usage: 0 bytes\n",
            "{'Blended Classifiers': ('Support Vector Machine', 'Naive Bayes'), 'Accuracy': 0.7587859424920128, 'Kappa': 0.6375874950317775, 'Precision': 0.7993851934758853, 'Recall': 0.7587859424920128, 'F1-Score': 0.7553557218700818, 'Confusion Matrix': array([[606,  21,   4],\n",
            "       [166, 453,  13],\n",
            "       [148, 101, 366]]), 'Execution Time (s)': 460.4739239215851, 'Total Classifiers': 5, 'Total Features': 9857, 'Training Data Size': 7512, 'Test Data Size': 1878, 'Random State': 42, 'Preprocessing': 1, 'SMOTE': 1, 'Total CPU Cores': 2, 'CPU Usage (%)': 65.4, 'Total RAM': 12982.625, 'Memory Usage': '0 bytes', 'Processor Type': 'x86_64', 'OS': 'Linux(5.15.120+)'}\n",
            "Blending the following classifiers: ('Support Vector Machine', 'Bagging')\n",
            "CPU Usage: 0.00%\n",
            "Memory Usage: 0 bytes\n",
            "{'Blended Classifiers': ('Support Vector Machine', 'Bagging'), 'Accuracy': 0.7715654952076677, 'Kappa': 0.6568097949920066, 'Precision': 0.8006151269355614, 'Recall': 0.7715654952076677, 'F1-Score': 0.7706178646590998, 'Confusion Matrix': array([[569,  62,   0],\n",
            "       [132, 490,  10],\n",
            "       [ 78, 147, 390]]), 'Execution Time (s)': 743.6676268577576, 'Total Classifiers': 5, 'Total Features': 9857, 'Training Data Size': 7512, 'Test Data Size': 1878, 'Random State': 42, 'Preprocessing': 1, 'SMOTE': 1, 'Total CPU Cores': 2, 'CPU Usage (%)': 0, 'Total RAM': 12982.625, 'Memory Usage': '0 bytes', 'Processor Type': 'x86_64', 'OS': 'Linux(5.15.120+)'}\n",
            "Blending the following classifiers: ('Naive Bayes', 'Bagging')\n",
            "CPU Usage: 0.00%\n",
            "Memory Usage: 11.30 MB\n",
            "{'Blended Classifiers': ('Naive Bayes', 'Bagging'), 'Accuracy': 0.7289669861554846, 'Kappa': 0.5927224481083299, 'Precision': 0.7751597436961634, 'Recall': 0.7289669861554846, 'F1-Score': 0.7230038427079516, 'Confusion Matrix': array([[605,  20,   6],\n",
            "       [179, 432,  21],\n",
            "       [178, 105, 332]]), 'Execution Time (s)': 315.47627806663513, 'Total Classifiers': 5, 'Total Features': 9857, 'Training Data Size': 7512, 'Test Data Size': 1878, 'Random State': 42, 'Preprocessing': 1, 'SMOTE': 1, 'Total CPU Cores': 2, 'CPU Usage (%)': 0, 'Total RAM': 12982.625, 'Memory Usage': '11.30 MB', 'Processor Type': 'x86_64', 'OS': 'Linux(5.15.120+)'}\n",
            "Blending the following classifiers: ('Logistic Regression', 'SDG', 'Support Vector Machine')\n",
            "CPU Usage: 0.00%\n",
            "Memory Usage: 0 bytes\n",
            "{'Blended Classifiers': ('Logistic Regression', 'SDG', 'Support Vector Machine'), 'Accuracy': 0.8242811501597445, 'Kappa': 0.7363504096791484, 'Precision': 0.8264907681208087, 'Recall': 0.8242811501597445, 'F1-Score': 0.8224220569727234, 'Confusion Matrix': array([[584,  37,  10],\n",
            "       [124, 450,  58],\n",
            "       [ 30,  71, 514]]), 'Execution Time (s)': 532.0241830348969, 'Total Classifiers': 5, 'Total Features': 9857, 'Training Data Size': 7512, 'Test Data Size': 1878, 'Random State': 42, 'Preprocessing': 1, 'SMOTE': 1, 'Total CPU Cores': 2, 'CPU Usage (%)': 0, 'Total RAM': 12982.625, 'Memory Usage': '0 bytes', 'Processor Type': 'x86_64', 'OS': 'Linux(5.15.120+)'}\n",
            "Blending the following classifiers: ('Logistic Regression', 'SDG', 'Naive Bayes')\n",
            "CPU Usage: 41.40%\n",
            "Memory Usage: 0 bytes\n",
            "{'Blended Classifiers': ('Logistic Regression', 'SDG', 'Naive Bayes'), 'Accuracy': 0.8115015974440895, 'Kappa': 0.7171939407087891, 'Precision': 0.8145618860558611, 'Recall': 0.8115015974440895, 'F1-Score': 0.8090268142494099, 'Confusion Matrix': array([[582,  33,  16],\n",
            "       [130, 435,  67],\n",
            "       [ 40,  68, 507]]), 'Execution Time (s)': 67.23250579833984, 'Total Classifiers': 5, 'Total Features': 9857, 'Training Data Size': 7512, 'Test Data Size': 1878, 'Random State': 42, 'Preprocessing': 1, 'SMOTE': 1, 'Total CPU Cores': 2, 'CPU Usage (%)': 41.400000000000006, 'Total RAM': 12982.625, 'Memory Usage': '0 bytes', 'Processor Type': 'x86_64', 'OS': 'Linux(5.15.120+)'}\n",
            "Blending the following classifiers: ('Logistic Regression', 'SDG', 'Bagging')\n",
            "CPU Usage: 68.70%\n",
            "Memory Usage: 26.82 MB\n",
            "{'Blended Classifiers': ('Logistic Regression', 'SDG', 'Bagging'), 'Accuracy': 0.8162939297124601, 'Kappa': 0.7243733728112716, 'Precision': 0.8193061228204999, 'Recall': 0.8162939297124601, 'F1-Score': 0.8142479729668504, 'Confusion Matrix': array([[581,  35,  15],\n",
            "       [128, 443,  61],\n",
            "       [ 38,  68, 509]]), 'Execution Time (s)': 376.2870693206787, 'Total Classifiers': 5, 'Total Features': 9857, 'Training Data Size': 7512, 'Test Data Size': 1878, 'Random State': 42, 'Preprocessing': 1, 'SMOTE': 1, 'Total CPU Cores': 2, 'CPU Usage (%)': 68.7, 'Total RAM': 12982.625, 'Memory Usage': '26.82 MB', 'Processor Type': 'x86_64', 'OS': 'Linux(5.15.120+)'}\n",
            "Blending the following classifiers: ('Logistic Regression', 'Support Vector Machine', 'Naive Bayes')\n",
            "CPU Usage: 68.40%\n",
            "Memory Usage: 0 bytes\n",
            "{'Blended Classifiers': ('Logistic Regression', 'Support Vector Machine', 'Naive Bayes'), 'Accuracy': 0.8168264110756124, 'Kappa': 0.7251330018073949, 'Precision': 0.8208470259151913, 'Recall': 0.8168264110756124, 'F1-Score': 0.8153089176229804, 'Confusion Matrix': array([[582,  37,  12],\n",
            "       [127, 452,  53],\n",
            "       [ 42,  73, 500]]), 'Execution Time (s)': 512.0458898544312, 'Total Classifiers': 5, 'Total Features': 9857, 'Training Data Size': 7512, 'Test Data Size': 1878, 'Random State': 42, 'Preprocessing': 1, 'SMOTE': 1, 'Total CPU Cores': 2, 'CPU Usage (%)': 68.4, 'Total RAM': 12982.625, 'Memory Usage': '0 bytes', 'Processor Type': 'x86_64', 'OS': 'Linux(5.15.120+)'}\n",
            "Blending the following classifiers: ('Logistic Regression', 'Support Vector Machine', 'Bagging')\n",
            "CPU Usage: 0.00%\n",
            "Memory Usage: 0 bytes\n",
            "{'Blended Classifiers': ('Logistic Regression', 'Support Vector Machine', 'Bagging'), 'Accuracy': 0.8136315228966986, 'Kappa': 0.7202923972128816, 'Precision': 0.8182334039986675, 'Recall': 0.8136315228966986, 'F1-Score': 0.8141307827602858, 'Confusion Matrix': array([[554,  68,   9],\n",
            "       [118, 476,  38],\n",
            "       [ 29,  88, 498]]), 'Execution Time (s)': 805.6380760669708, 'Total Classifiers': 5, 'Total Features': 9857, 'Training Data Size': 7512, 'Test Data Size': 1878, 'Random State': 42, 'Preprocessing': 1, 'SMOTE': 1, 'Total CPU Cores': 2, 'CPU Usage (%)': 0, 'Total RAM': 12982.625, 'Memory Usage': '0 bytes', 'Processor Type': 'x86_64', 'OS': 'Linux(5.15.120+)'}\n",
            "Blending the following classifiers: ('Logistic Regression', 'Naive Bayes', 'Bagging')\n",
            "CPU Usage: 69.60%\n",
            "Memory Usage: 31.46 MB\n",
            "{'Blended Classifiers': ('Logistic Regression', 'Naive Bayes', 'Bagging'), 'Accuracy': 0.8035143769968051, 'Kappa': 0.7051533136224835, 'Precision': 0.8089155639686919, 'Recall': 0.8035143769968051, 'F1-Score': 0.8014707648488582, 'Confusion Matrix': array([[583,  34,  14],\n",
            "       [132, 439,  61],\n",
            "       [ 56,  72, 487]]), 'Execution Time (s)': 330.5233302116394, 'Total Classifiers': 5, 'Total Features': 9857, 'Training Data Size': 7512, 'Test Data Size': 1878, 'Random State': 42, 'Preprocessing': 1, 'SMOTE': 1, 'Total CPU Cores': 2, 'CPU Usage (%)': 69.6, 'Total RAM': 12982.625, 'Memory Usage': '31.46 MB', 'Processor Type': 'x86_64', 'OS': 'Linux(5.15.120+)'}\n",
            "Blending the following classifiers: ('SDG', 'Support Vector Machine', 'Naive Bayes')\n",
            "CPU Usage: 65.80%\n",
            "Memory Usage: 0 bytes\n",
            "{'Blended Classifiers': ('SDG', 'Support Vector Machine', 'Naive Bayes'), 'Accuracy': 0.8264110756123536, 'Kappa': 0.7395185696215618, 'Precision': 0.8331442798913506, 'Recall': 0.8264110756123536, 'F1-Score': 0.8243352832704054, 'Confusion Matrix': array([[599,  25,   7],\n",
            "       [133, 447,  52],\n",
            "       [ 46,  63, 506]]), 'Execution Time (s)': 479.2835388183594, 'Total Classifiers': 5, 'Total Features': 9857, 'Training Data Size': 7512, 'Test Data Size': 1878, 'Random State': 42, 'Preprocessing': 1, 'SMOTE': 1, 'Total CPU Cores': 2, 'CPU Usage (%)': 65.8, 'Total RAM': 12982.625, 'Memory Usage': '0 bytes', 'Processor Type': 'x86_64', 'OS': 'Linux(5.15.120+)'}\n",
            "Blending the following classifiers: ('SDG', 'Support Vector Machine', 'Bagging')\n",
            "CPU Usage: 0.00%\n",
            "Memory Usage: 0 bytes\n",
            "{'Blended Classifiers': ('SDG', 'Support Vector Machine', 'Bagging'), 'Accuracy': 0.8237486687965921, 'Kappa': 0.7354748357297785, 'Precision': 0.8288339441454006, 'Recall': 0.8237486687965921, 'F1-Score': 0.8238997038784541, 'Confusion Matrix': array([[570,  59,   2],\n",
            "       [124, 472,  36],\n",
            "       [ 26,  84, 505]]), 'Execution Time (s)': 777.579479932785, 'Total Classifiers': 5, 'Total Features': 9857, 'Training Data Size': 7512, 'Test Data Size': 1878, 'Random State': 42, 'Preprocessing': 1, 'SMOTE': 1, 'Total CPU Cores': 2, 'CPU Usage (%)': 0, 'Total RAM': 12982.625, 'Memory Usage': '0 bytes', 'Processor Type': 'x86_64', 'OS': 'Linux(5.15.120+)'}\n",
            "Blending the following classifiers: ('SDG', 'Naive Bayes', 'Bagging')\n",
            "CPU Usage: 64.10%\n",
            "Memory Usage: 19.76 MB\n",
            "{'Blended Classifiers': ('SDG', 'Naive Bayes', 'Bagging'), 'Accuracy': 0.8189563365282215, 'Kappa': 0.7283523608354658, 'Precision': 0.8284371049247327, 'Recall': 0.8189563365282215, 'F1-Score': 0.8162025847588215, 'Confusion Matrix': array([[601,  23,   7],\n",
            "       [144, 429,  59],\n",
            "       [ 54,  53, 508]]), 'Execution Time (s)': 305.9218726158142, 'Total Classifiers': 5, 'Total Features': 9857, 'Training Data Size': 7512, 'Test Data Size': 1878, 'Random State': 42, 'Preprocessing': 1, 'SMOTE': 1, 'Total CPU Cores': 2, 'CPU Usage (%)': 64.1, 'Total RAM': 12982.625, 'Memory Usage': '19.76 MB', 'Processor Type': 'x86_64', 'OS': 'Linux(5.15.120+)'}\n",
            "Blending the following classifiers: ('Support Vector Machine', 'Naive Bayes', 'Bagging')\n",
            "CPU Usage: 0.00%\n",
            "Memory Usage: 0 bytes\n",
            "{'Blended Classifiers': ('Support Vector Machine', 'Naive Bayes', 'Bagging'), 'Accuracy': 0.8013844515441959, 'Kappa': 0.7018609906982386, 'Precision': 0.8092790985936616, 'Recall': 0.8013844515441959, 'F1-Score': 0.8020218267251891, 'Confusion Matrix': array([[552,  70,   9],\n",
            "       [120, 477,  35],\n",
            "       [ 55,  84, 476]]), 'Execution Time (s)': 732.2666261196136, 'Total Classifiers': 5, 'Total Features': 9857, 'Training Data Size': 7512, 'Test Data Size': 1878, 'Random State': 42, 'Preprocessing': 1, 'SMOTE': 1, 'Total CPU Cores': 2, 'CPU Usage (%)': 0, 'Total RAM': 12982.625, 'Memory Usage': '0 bytes', 'Processor Type': 'x86_64', 'OS': 'Linux(5.15.120+)'}\n",
            "Blending the following classifiers: ('Logistic Regression', 'SDG', 'Support Vector Machine', 'Naive Bayes')\n",
            "CPU Usage: 0.00%\n",
            "Memory Usage: 0 bytes\n",
            "{'Blended Classifiers': ('Logistic Regression', 'SDG', 'Support Vector Machine', 'Naive Bayes'), 'Accuracy': 0.8269435569755058, 'Kappa': 0.7403098581841219, 'Precision': 0.8312116397763929, 'Recall': 0.8269435569755058, 'F1-Score': 0.8250191513921045, 'Confusion Matrix': array([[596,  28,   7],\n",
            "       [128, 453,  51],\n",
            "       [ 35,  76, 504]]), 'Execution Time (s)': 534.5495755672455, 'Total Classifiers': 5, 'Total Features': 9857, 'Training Data Size': 7512, 'Test Data Size': 1878, 'Random State': 42, 'Preprocessing': 1, 'SMOTE': 1, 'Total CPU Cores': 2, 'CPU Usage (%)': 0, 'Total RAM': 12982.625, 'Memory Usage': '0 bytes', 'Processor Type': 'x86_64', 'OS': 'Linux(5.15.120+)'}\n",
            "Blending the following classifiers: ('Logistic Regression', 'SDG', 'Support Vector Machine', 'Bagging')\n",
            "CPU Usage: 0.00%\n",
            "Memory Usage: 0 bytes\n",
            "{'Blended Classifiers': ('Logistic Regression', 'SDG', 'Support Vector Machine', 'Bagging'), 'Accuracy': 0.8264110756123536, 'Kappa': 0.7394572686054834, 'Precision': 0.8322211917625882, 'Recall': 0.8264110756123536, 'F1-Score': 0.8254237087838296, 'Confusion Matrix': array([[592,  32,   7],\n",
            "       [127, 469,  36],\n",
            "       [ 33,  91, 491]]), 'Execution Time (s)': 826.0814638137817, 'Total Classifiers': 5, 'Total Features': 9857, 'Training Data Size': 7512, 'Test Data Size': 1878, 'Random State': 42, 'Preprocessing': 1, 'SMOTE': 1, 'Total CPU Cores': 2, 'CPU Usage (%)': 0, 'Total RAM': 12982.625, 'Memory Usage': '0 bytes', 'Processor Type': 'x86_64', 'OS': 'Linux(5.15.120+)'}\n",
            "Blending the following classifiers: ('Logistic Regression', 'SDG', 'Naive Bayes', 'Bagging')\n",
            "CPU Usage: 0.00%\n",
            "Memory Usage: 13.77 MB\n",
            "{'Blended Classifiers': ('Logistic Regression', 'SDG', 'Naive Bayes', 'Bagging'), 'Accuracy': 0.8115015974440895, 'Kappa': 0.7171354610761933, 'Precision': 0.8172702022750298, 'Recall': 0.8115015974440895, 'F1-Score': 0.8092551836782524, 'Confusion Matrix': array([[592,  25,  14],\n",
            "       [136, 441,  55],\n",
            "       [ 48,  76, 491]]), 'Execution Time (s)': 337.24746584892273, 'Total Classifiers': 5, 'Total Features': 9857, 'Training Data Size': 7512, 'Test Data Size': 1878, 'Random State': 42, 'Preprocessing': 1, 'SMOTE': 1, 'Total CPU Cores': 2, 'CPU Usage (%)': 0, 'Total RAM': 12982.625, 'Memory Usage': '13.77 MB', 'Processor Type': 'x86_64', 'OS': 'Linux(5.15.120+)'}\n",
            "Blending the following classifiers: ('Logistic Regression', 'Support Vector Machine', 'Naive Bayes', 'Bagging')\n",
            "CPU Usage: 66.90%\n",
            "Memory Usage: 0 bytes\n",
            "{'Blended Classifiers': ('Logistic Regression', 'Support Vector Machine', 'Naive Bayes', 'Bagging'), 'Accuracy': 0.820021299254526, 'Kappa': 0.7298306365939364, 'Precision': 0.8284588258875988, 'Recall': 0.820021299254526, 'F1-Score': 0.8194070331579337, 'Confusion Matrix': array([[588,  35,   8],\n",
            "       [127, 475,  30],\n",
            "       [ 46,  92, 477]]), 'Execution Time (s)': 758.4879686832428, 'Total Classifiers': 5, 'Total Features': 9857, 'Training Data Size': 7512, 'Test Data Size': 1878, 'Random State': 42, 'Preprocessing': 1, 'SMOTE': 1, 'Total CPU Cores': 2, 'CPU Usage (%)': 66.9, 'Total RAM': 12982.625, 'Memory Usage': '0 bytes', 'Processor Type': 'x86_64', 'OS': 'Linux(5.15.120+)'}\n",
            "Blending the following classifiers: ('SDG', 'Support Vector Machine', 'Naive Bayes', 'Bagging')\n",
            "CPU Usage: 64.20%\n",
            "Memory Usage: 0 bytes\n",
            "{'Blended Classifiers': ('SDG', 'Support Vector Machine', 'Naive Bayes', 'Bagging'), 'Accuracy': 0.8232161874334398, 'Kappa': 0.7346181839850516, 'Precision': 0.8343675339713529, 'Recall': 0.8232161874334398, 'F1-Score': 0.8222531217784883, 'Confusion Matrix': array([[601,  25,   5],\n",
            "       [137, 468,  27],\n",
            "       [ 51,  87, 477]]), 'Execution Time (s)': 771.717485666275, 'Total Classifiers': 5, 'Total Features': 9857, 'Training Data Size': 7512, 'Test Data Size': 1878, 'Random State': 42, 'Preprocessing': 1, 'SMOTE': 1, 'Total CPU Cores': 2, 'CPU Usage (%)': 64.2, 'Total RAM': 12982.625, 'Memory Usage': '0 bytes', 'Processor Type': 'x86_64', 'OS': 'Linux(5.15.120+)'}\n",
            "Blending the following classifiers: ('Logistic Regression', 'SDG', 'Support Vector Machine', 'Naive Bayes', 'Bagging')\n",
            "CPU Usage: 0.00%\n",
            "Memory Usage: 0 bytes\n",
            "{'Blended Classifiers': ('Logistic Regression', 'SDG', 'Support Vector Machine', 'Naive Bayes', 'Bagging'), 'Accuracy': 0.8237486687965921, 'Kappa': 0.7355219928198172, 'Precision': 0.8273122484053352, 'Recall': 0.8237486687965921, 'F1-Score': 0.822144800394977, 'Confusion Matrix': array([[587,  37,   7],\n",
            "       [127, 453,  52],\n",
            "       [ 34,  74, 507]]), 'Execution Time (s)': 814.447452545166, 'Total Classifiers': 5, 'Total Features': 9857, 'Training Data Size': 7512, 'Test Data Size': 1878, 'Random State': 42, 'Preprocessing': 1, 'SMOTE': 1, 'Total CPU Cores': 2, 'CPU Usage (%)': 0, 'Total RAM': 12982.625, 'Memory Usage': '0 bytes', 'Processor Type': 'x86_64', 'OS': 'Linux(5.15.120+)'}\n",
            "                                  Blended Classifiers  Accuracy     Kappa  \\\n",
            "0   (Logistic Regression, SDG, Support Vector Mach...  0.826944  0.740310   \n",
            "1          (SDG, Support Vector Machine, Naive Bayes)  0.826411  0.739519   \n",
            "2   (Logistic Regression, SDG, Support Vector Mach...  0.826411  0.739457   \n",
            "3                       (SDG, Support Vector Machine)  0.825879  0.738556   \n",
            "4   (Logistic Regression, SDG, Support Vector Mach...  0.824281  0.736350   \n",
            "5              (SDG, Support Vector Machine, Bagging)  0.823749  0.735475   \n",
            "6   (Logistic Regression, SDG, Support Vector Mach...  0.823749  0.735522   \n",
            "7   (SDG, Support Vector Machine, Naive Bayes, Bag...  0.823216  0.734618   \n",
            "8       (Logistic Regression, Support Vector Machine)  0.821619  0.732154   \n",
            "9   (Logistic Regression, Support Vector Machine, ...  0.820021  0.729831   \n",
            "10                        (SDG, Naive Bayes, Bagging)  0.818956  0.728352   \n",
            "11                         (Logistic Regression, SDG)  0.818424  0.727528   \n",
            "12  (Logistic Regression, Support Vector Machine, ...  0.816826  0.725133   \n",
            "13                (Logistic Regression, SDG, Bagging)  0.816294  0.724373   \n",
            "14  (Logistic Regression, Support Vector Machine, ...  0.813632  0.720292   \n",
            "15   (Logistic Regression, SDG, Naive Bayes, Bagging)  0.811502  0.717135   \n",
            "16            (Logistic Regression, SDG, Naive Bayes)  0.811502  0.717194   \n",
            "17        (Logistic Regression, Naive Bayes, Bagging)  0.803514  0.705153   \n",
            "18     (Support Vector Machine, Naive Bayes, Bagging)  0.801384  0.701861   \n",
            "19                                     (SDG, Bagging)  0.783813  0.675328   \n",
            "20                     (Logistic Regression, Bagging)  0.781683  0.672149   \n",
            "21                  (Support Vector Machine, Bagging)  0.771565  0.656810   \n",
            "22              (Support Vector Machine, Naive Bayes)  0.758786  0.637587   \n",
            "23                                 (SDG, Naive Bayes)  0.757721  0.636148   \n",
            "24                 (Logistic Regression, Naive Bayes)  0.752929  0.628968   \n",
            "25                             (Naive Bayes, Bagging)  0.728967  0.592722   \n",
            "\n",
            "    Precision    Recall  F1-Score  \\\n",
            "0    0.831212  0.826944  0.825019   \n",
            "1    0.833144  0.826411  0.824335   \n",
            "2    0.832221  0.826411  0.825424   \n",
            "3    0.839825  0.825879  0.824692   \n",
            "4    0.826491  0.824281  0.822422   \n",
            "5    0.828834  0.823749  0.823900   \n",
            "6    0.827312  0.823749  0.822145   \n",
            "7    0.834368  0.823216  0.822253   \n",
            "8    0.834726  0.821619  0.821350   \n",
            "9    0.828459  0.820021  0.819407   \n",
            "10   0.828437  0.818956  0.816203   \n",
            "11   0.822975  0.818424  0.815992   \n",
            "12   0.820847  0.816826  0.815309   \n",
            "13   0.819306  0.816294  0.814248   \n",
            "14   0.818233  0.813632  0.814131   \n",
            "15   0.817270  0.811502  0.809255   \n",
            "16   0.814562  0.811502  0.809027   \n",
            "17   0.808916  0.803514  0.801471   \n",
            "18   0.809279  0.801384  0.802022   \n",
            "19   0.802894  0.783813  0.780759   \n",
            "20   0.796555  0.781683  0.779815   \n",
            "21   0.800615  0.771565  0.770618   \n",
            "22   0.799385  0.758786  0.755356   \n",
            "23   0.793733  0.757721  0.754927   \n",
            "24   0.782370  0.752929  0.750325   \n",
            "25   0.775160  0.728967  0.723004   \n",
            "\n",
            "                                   Confusion Matrix  Execution Time (s)  \\\n",
            "0     [[596, 28, 7], [128, 453, 51], [35, 76, 504]]          534.549576   \n",
            "1     [[599, 25, 7], [133, 447, 52], [46, 63, 506]]          479.283539   \n",
            "2     [[592, 32, 7], [127, 469, 36], [33, 91, 491]]          826.081464   \n",
            "3    [[612, 18, 1], [138, 478, 16], [40, 114, 461]]          483.153162   \n",
            "4    [[584, 37, 10], [124, 450, 58], [30, 71, 514]]          532.024183   \n",
            "5     [[570, 59, 2], [124, 472, 36], [26, 84, 505]]          777.579480   \n",
            "6     [[587, 37, 7], [127, 453, 52], [34, 74, 507]]          814.447453   \n",
            "7     [[601, 25, 5], [137, 468, 27], [51, 87, 477]]          771.717486   \n",
            "8    [[594, 36, 1], [125, 490, 17], [36, 120, 459]]          536.613013   \n",
            "9     [[588, 35, 8], [127, 475, 30], [46, 92, 477]]          758.487969   \n",
            "10    [[601, 23, 7], [144, 429, 59], [54, 53, 508]]          305.921873   \n",
            "11   [[597, 24, 10], [133, 443, 56], [39, 79, 497]]           69.966457   \n",
            "12   [[582, 37, 12], [127, 452, 53], [42, 73, 500]]          512.045890   \n",
            "13   [[581, 35, 15], [128, 443, 61], [38, 68, 509]]          376.287069   \n",
            "14    [[554, 68, 9], [118, 476, 38], [29, 88, 498]]          805.638076   \n",
            "15   [[592, 25, 14], [136, 441, 55], [48, 76, 491]]          337.247466   \n",
            "16   [[582, 33, 16], [130, 435, 67], [40, 68, 507]]           67.232506   \n",
            "17   [[583, 34, 14], [132, 439, 61], [56, 72, 487]]          330.523330   \n",
            "18    [[552, 70, 9], [120, 477, 35], [55, 84, 476]]          732.266626   \n",
            "19   [[609, 21, 1], [155, 447, 30], [76, 123, 416]]          297.349533   \n",
            "20   [[586, 37, 8], [138, 463, 31], [71, 125, 419]]          357.460369   \n",
            "21   [[569, 62, 0], [132, 490, 10], [78, 147, 390]]          743.667627   \n",
            "22  [[606, 21, 4], [166, 453, 13], [148, 101, 366]]          460.473924   \n",
            "23   [[611, 17, 3], [176, 414, 42], [152, 65, 398]]           17.659965   \n",
            "24   [[597, 25, 9], [167, 420, 45], [147, 71, 397]]           51.804734   \n",
            "25  [[605, 20, 6], [179, 432, 21], [178, 105, 332]]          315.476278   \n",
            "\n",
            "    Total Classifiers  Total Features  ...  Test Data Size  Random State  \\\n",
            "0                   5            9857  ...            1878            42   \n",
            "1                   5            9857  ...            1878            42   \n",
            "2                   5            9857  ...            1878            42   \n",
            "3                   5            9857  ...            1878            42   \n",
            "4                   5            9857  ...            1878            42   \n",
            "5                   5            9857  ...            1878            42   \n",
            "6                   5            9857  ...            1878            42   \n",
            "7                   5            9857  ...            1878            42   \n",
            "8                   5            9857  ...            1878            42   \n",
            "9                   5            9857  ...            1878            42   \n",
            "10                  5            9857  ...            1878            42   \n",
            "11                  5            9857  ...            1878            42   \n",
            "12                  5            9857  ...            1878            42   \n",
            "13                  5            9857  ...            1878            42   \n",
            "14                  5            9857  ...            1878            42   \n",
            "15                  5            9857  ...            1878            42   \n",
            "16                  5            9857  ...            1878            42   \n",
            "17                  5            9857  ...            1878            42   \n",
            "18                  5            9857  ...            1878            42   \n",
            "19                  5            9857  ...            1878            42   \n",
            "20                  5            9857  ...            1878            42   \n",
            "21                  5            9857  ...            1878            42   \n",
            "22                  5            9857  ...            1878            42   \n",
            "23                  5            9857  ...            1878            42   \n",
            "24                  5            9857  ...            1878            42   \n",
            "25                  5            9857  ...            1878            42   \n",
            "\n",
            "    Preprocessing  SMOTE  Total CPU Cores  CPU Usage (%)  Total RAM  \\\n",
            "0               1      1                2            0.0  12982.625   \n",
            "1               1      1                2           65.8  12982.625   \n",
            "2               1      1                2            0.0  12982.625   \n",
            "3               1      1                2           67.5  12982.625   \n",
            "4               1      1                2            0.0  12982.625   \n",
            "5               1      1                2            0.0  12982.625   \n",
            "6               1      1                2            0.0  12982.625   \n",
            "7               1      1                2           64.2  12982.625   \n",
            "8               1      1                2            0.0  12982.625   \n",
            "9               1      1                2           66.9  12982.625   \n",
            "10              1      1                2           64.1  12982.625   \n",
            "11              1      1                2           68.6  12982.625   \n",
            "12              1      1                2           68.4  12982.625   \n",
            "13              1      1                2           68.7  12982.625   \n",
            "14              1      1                2            0.0  12982.625   \n",
            "15              1      1                2            0.0  12982.625   \n",
            "16              1      1                2           41.4  12982.625   \n",
            "17              1      1                2           69.6  12982.625   \n",
            "18              1      1                2            0.0  12982.625   \n",
            "19              1      1                2           66.8  12982.625   \n",
            "20              1      1                2            0.0  12982.625   \n",
            "21              1      1                2            0.0  12982.625   \n",
            "22              1      1                2           65.4  12982.625   \n",
            "23              1      1                2           64.7  12982.625   \n",
            "24              1      1                2            0.0  12982.625   \n",
            "25              1      1                2            0.0  12982.625   \n",
            "\n",
            "    Memory Usage Processor Type                OS  \n",
            "0        0 bytes         x86_64  Linux(5.15.120+)  \n",
            "1        0 bytes         x86_64  Linux(5.15.120+)  \n",
            "2        0 bytes         x86_64  Linux(5.15.120+)  \n",
            "3        0 bytes         x86_64  Linux(5.15.120+)  \n",
            "4        0 bytes         x86_64  Linux(5.15.120+)  \n",
            "5        0 bytes         x86_64  Linux(5.15.120+)  \n",
            "6        0 bytes         x86_64  Linux(5.15.120+)  \n",
            "7        0 bytes         x86_64  Linux(5.15.120+)  \n",
            "8        0 bytes         x86_64  Linux(5.15.120+)  \n",
            "9        0 bytes         x86_64  Linux(5.15.120+)  \n",
            "10      19.76 MB         x86_64  Linux(5.15.120+)  \n",
            "11       0 bytes         x86_64  Linux(5.15.120+)  \n",
            "12       0 bytes         x86_64  Linux(5.15.120+)  \n",
            "13      26.82 MB         x86_64  Linux(5.15.120+)  \n",
            "14       0 bytes         x86_64  Linux(5.15.120+)  \n",
            "15      13.77 MB         x86_64  Linux(5.15.120+)  \n",
            "16       0 bytes         x86_64  Linux(5.15.120+)  \n",
            "17      31.46 MB         x86_64  Linux(5.15.120+)  \n",
            "18       0 bytes         x86_64  Linux(5.15.120+)  \n",
            "19      21.84 MB         x86_64  Linux(5.15.120+)  \n",
            "20      17.08 MB         x86_64  Linux(5.15.120+)  \n",
            "21       0 bytes         x86_64  Linux(5.15.120+)  \n",
            "22       0 bytes         x86_64  Linux(5.15.120+)  \n",
            "23       0 bytes         x86_64  Linux(5.15.120+)  \n",
            "24       0 bytes         x86_64  Linux(5.15.120+)  \n",
            "25      11.30 MB         x86_64  Linux(5.15.120+)  \n",
            "\n",
            "[26 rows x 21 columns]\n",
            "Results have been appended to /content/Drive/MyDrive/python/results/blended_classifiers/lm_2023-10-03_blended_classifier_results.csv in Google Drive.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Evaluating\n",
        "\n",
        "for r in range(2, len(classifiers) + 1):\n",
        "    combinations_list = list(combinations(classifiers, r))\n",
        "\n",
        "    # Iterate through each combination and call the train_and_evaluate_blended_classifier method\n",
        "    for combo in combinations_list:\n",
        "        num_classifiers = len(combo)  # Get the number of classifiers in the current combination\n",
        "        results = train_and_evaluate_blended_classifier(combo, additional_info, num_classifiers)\n",
        "        results_list.append(results)\n",
        "\n",
        "# Sort the blended results based on accuracy in descending order\n",
        "sorted_results = sorted(results_list, key=lambda x: (x['Accuracy'], x['Precision'], x['Recall'], x['F1-Score']), reverse=True)\n",
        "\n",
        "# Save the sorted_results and additional information to a DataFrame\n",
        "sorted_results_df = pd.DataFrame(sorted_results)\n",
        "print(sorted_results_df)\n",
        "\n",
        "# Set the time zone to Pakistan Standard Time (PST)\n",
        "pakistan_tz = pytz.timezone('Asia/Karachi')\n",
        "\n",
        "# Get the current date in Pakistan's time zone\n",
        "current_date = datetime.now(pakistan_tz).strftime('%Y-%m-%d')\n",
        "\n",
        "# Append the new results to the existing file or create a new file if it doesn't exist\n",
        "file_name = f\"lm_{current_date}_blended_classifier_results.csv\"\n",
        "file_path = f\"/content/Drive/MyDrive/python/results/blended_classifiers/{file_name}\"\n",
        "\n",
        "try:\n",
        "    if os.path.exists(file_path):\n",
        "        existing_results_df = pd.read_csv(file_path)\n",
        "        final_df = pd.concat([existing_results_df, sorted_results_df], ignore_index=True)\n",
        "    else:\n",
        "        final_df = sorted_results_df\n",
        "\n",
        "    final_df.to_csv(file_path, index=False)\n",
        "    print(f\"Results have been appended to {file_path} in Google Drive.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while saving the file: {str(e)}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}