{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ranafaraz/financial_sentiment_analysis/blob/main/TFIDF_Loughran_McDonald_Word2Vec_Blending_ML_Classifiers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtK82S9batdu",
        "outputId": "2eb6442e-653a-4320-fbee-cf55b9f9e09a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/Drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/Drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Fe9WUxpbbpQK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9826a92-b998-4bd3-d623-81cd0a467469"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting num2words\n",
            "  Downloading num2words-0.5.12-py3-none-any.whl (125 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/125.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m112.6/125.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6.2 (from num2words)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=c9ff8311a04f339c437ae1333a4dcc924ee648bd524b9f7f15f74a473b90a464\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: docopt, num2words\n",
            "Successfully installed docopt-0.6.2 num2words-0.5.12\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import psutil\n",
        "import platform\n",
        "import scipy.sparse as sp\n",
        "from scipy.sparse import csr_matrix, hstack\n",
        "# import joblib\n",
        "# from itertools import permutations\n",
        "from itertools import combinations, chain\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import pytz # timezone\n",
        "from datetime import datetime\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sys\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from tqdm import tqdm\n",
        "import multiprocessing\n",
        "from multiprocessing import Process, Queue\n",
        "\n",
        "\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, cohen_kappa_score\n",
        "# from sklearn.base import clone\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "# from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "!pip install num2words\n",
        "from num2words import num2words\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zYLFMIwAVZk4"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/Drive/MyDrive/python/data.csv\")\n",
        "# custom_stopwords_df = pd.read_csv(\"/content/Drive/MyDrive/python/financial_sentiment_analysis/custom_stopwords.csv\")  # StopWord List\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Python module that will load the Loughran-McDonald Master and its components (and optionally separate sentiment dictionaries).\n",
        "\n",
        "But this technique doesn't worked well in our case. It only added few new features."
      ],
      "metadata": {
        "id": "xGX-KhJA3R08"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AAZVL9xBgOJO"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "Routine to load master dicitonary\n",
        "Version for LM 2020 Master Dictionary\n",
        "\n",
        "Bill McDonald\n",
        "Date: 201510 Updated: 202201\n",
        "\"\"\"\n",
        "\n",
        "import datetime as dt\n",
        "import sys\n",
        "\n",
        "\n",
        "def load_masterdictionary(file_path, print_flag=False, f_log=None, get_other=False):\n",
        "    start_local = dt.datetime.now()\n",
        "    # Setup dictionaries\n",
        "    _master_dictionary = {}\n",
        "    _sentiment_categories = ['negative', 'positive', 'uncertainty', 'litigious',\n",
        "                             'strong_modal', 'weak_modal', 'constraining']\n",
        "    _sentiment_dictionaries = dict()\n",
        "    for sentiment in _sentiment_categories:\n",
        "        _sentiment_dictionaries[sentiment] = dict()\n",
        "\n",
        "    # Load slightly modified common stopwords.\n",
        "    # Dropped from traditional: A, I, S, T, DON, WILL, AGAINST\n",
        "    # Added: AMONG\n",
        "    _stopwords = ['ME', 'MY', 'MYSELF', 'WE', 'OUR', 'OURS', 'OURSELVES', 'YOU', 'YOUR', 'YOURS',\n",
        "                  'YOURSELF', 'YOURSELVES', 'HE', 'HIM', 'HIS', 'HIMSELF', 'SHE', 'HER', 'HERS', 'HERSELF',\n",
        "                  'IT', 'ITS', 'ITSELF', 'THEY', 'THEM', 'THEIR', 'THEIRS', 'THEMSELVES', 'WHAT', 'WHICH',\n",
        "                  'WHO', 'WHOM', 'THIS', 'THAT', 'THESE', 'THOSE', 'AM', 'IS', 'ARE', 'WAS', 'WERE', 'BE',\n",
        "                  'BEEN', 'BEING', 'HAVE', 'HAS', 'HAD', 'HAVING', 'DO', 'DOES', 'DID', 'DOING', 'AN',\n",
        "                  'THE', 'AND', 'BUT', 'IF', 'OR', 'BECAUSE', 'AS', 'UNTIL', 'WHILE', 'OF', 'AT', 'BY',\n",
        "                  'FOR', 'WITH', 'ABOUT', 'BETWEEN', 'INTO', 'THROUGH', 'DURING', 'BEFORE',\n",
        "                  'AFTER', 'ABOVE', 'BELOW', 'TO', 'FROM', 'UP', 'DOWN', 'IN', 'OUT', 'ON', 'OFF', 'OVER',\n",
        "                  'UNDER', 'AGAIN', 'FURTHER', 'THEN', 'ONCE', 'HERE', 'THERE', 'WHEN', 'WHERE', 'WHY',\n",
        "                  'HOW', 'ALL', 'ANY', 'BOTH', 'EACH', 'FEW', 'MORE', 'MOST', 'OTHER', 'SOME', 'SUCH',\n",
        "                  'NO', 'NOR', 'NOT', 'ONLY', 'OWN', 'SAME', 'SO', 'THAN', 'TOO', 'VERY', 'CAN',\n",
        "                  'JUST', 'SHOULD', 'NOW', 'AMONG']\n",
        "\n",
        "    # Loop thru words and load dictionaries\n",
        "    with open(file_path) as f:\n",
        "        _total_documents = 0\n",
        "        _md_header = f.readline()  # Consume header line\n",
        "        print()\n",
        "        for line in f:\n",
        "            cols = line.rstrip('\\n').split(',')\n",
        "            word = cols[0]\n",
        "            _master_dictionary[word] = MasterDictionary(cols, _stopwords)\n",
        "            for sentiment in _sentiment_categories:\n",
        "                if getattr(_master_dictionary[word], sentiment):\n",
        "                    _sentiment_dictionaries[sentiment][word] = 0\n",
        "            _total_documents += _master_dictionary[cols[0]].doc_count\n",
        "            if len(_master_dictionary) % 5000 == 0 and print_flag:\n",
        "                print(f'\\r ...Loading Master Dictionary {len(_master_dictionary):,}', end='', flush=True)\n",
        "\n",
        "    if print_flag:\n",
        "        print('\\r', end='')  # clear line\n",
        "        print(f'\\nMaster Dictionary loaded from file:\\n  {file_path}\\n')\n",
        "        print(f'  master_dictionary has {len(_master_dictionary):,} words.\\n')\n",
        "\n",
        "    if f_log:\n",
        "        try:\n",
        "            f_log.write('\\n\\n  FUNCTION: load_masterdictionary' +\n",
        "                        '(file_path, print_flag, f_log, get_other)\\n')\n",
        "            f_log.write(f'\\n    file_path  = {file_path}')\n",
        "            f_log.write(f'\\n    print_flag = {print_flag}')\n",
        "            f_log.write(f'\\n    f_log      = {f_log.name}')\n",
        "            f_log.write(f'\\n    get_other  = {get_other}')\n",
        "            f_log.write(f'\\n\\n    {len(_master_dictionary):,} words loaded in master_dictionary.\\n')\n",
        "            f_log.write(f'\\n    Sentiment:')\n",
        "            for sentiment in _sentiment_categories:\n",
        "                f_log.write(f'\\n      {sentiment:13}: {len(_sentiment_dictionaries[sentiment]):8,}')\n",
        "            f_log.write(f'\\n\\n  END FUNCTION: load_masterdictionary: {(dt.datetime.now()-start_local)}')\n",
        "        except Exception as e:\n",
        "            print('Log file in load_masterdictionary is not available for writing')\n",
        "            print(f'Error = {e}')\n",
        "\n",
        "    if get_other:\n",
        "        return _master_dictionary, _md_header, _sentiment_categories, _sentiment_dictionaries, _stopwords, _total_documents\n",
        "    else:\n",
        "        return _master_dictionary\n",
        "\n",
        "\n",
        "class MasterDictionary:\n",
        "    def __init__(self, cols, _stopwords):\n",
        "        for ptr, col in enumerate(cols):\n",
        "            if col == '':\n",
        "                cols[ptr] = '0'\n",
        "        try:\n",
        "            self.word = cols[0].upper()\n",
        "            self.sequence_number = int(cols[1])\n",
        "            self.word_count = int(cols[2])\n",
        "            self.word_proportion = float(cols[3])\n",
        "            self.average_proportion = float(cols[4])\n",
        "            self.std_dev_prop = float(cols[5])\n",
        "            self.doc_count = int(cols[6])\n",
        "            self.negative = int(cols[7])\n",
        "            self.positive = int(cols[8])\n",
        "            self.uncertainty = int(cols[9])\n",
        "            self.litigious = int(cols[10])\n",
        "            self.strong_modal = int(cols[11])\n",
        "            self.weak_modal = int(cols[12])\n",
        "            self.constraining = int(cols[13])\n",
        "            self.syllables = int(cols[14])\n",
        "            self.source = cols[15]\n",
        "            if self.word in _stopwords:\n",
        "                self.stopword = True\n",
        "            else:\n",
        "                self.stopword = False\n",
        "        except:\n",
        "            print('ERROR in class MasterDictionary')\n",
        "            print(f'word = {cols[0]} : seqnum = {cols[1]}')\n",
        "            quit()\n",
        "        return\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the method to load Loughran-McDonald Master Dictionary\n",
        "if __name__ == '__main__':\n",
        "    start = dt.datetime.now()\n",
        "    print(f'\\n\\n{start.strftime(\"%c\")}\\nPROGRAM NAME: {sys.argv[0]}\\n')\n",
        "    f_log = open('/content/Drive/MyDrive/python/financial_sentiment_analysis/Load_MD_Logfile.txt', 'w')\n",
        "    md = (r\"/content/Drive/MyDrive/python/financial_sentiment_analysis/Loughran-McDonald_MasterDictionary_1993-2021.csv\")\n",
        "    master_dictionary, md_header, sentiment_categories, sentiment_dictionaries, stopwords, total_documents = \\\n",
        "        load_masterdictionary(md, True, f_log, True)\n",
        "    print(f'\\n\\nRuntime: {(dt.datetime.now()-start)}')\n",
        "    print(f'\\nNormal termination.\\n{dt.datetime.now().strftime(\"%c\")}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MW9ZzBB9QEHt",
        "outputId": "64203948-d5d0-4a5b-9b62-43e8be91bdee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Sat Sep 30 17:07:15 2023\n",
            "PROGRAM NAME: /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\n",
            "\n",
            "\n",
            " ...Loading Master Dictionary 85,000\n",
            "Master Dictionary loaded from file:\n",
            "  /content/Drive/MyDrive/python/financial_sentiment_analysis/Loughran-McDonald_MasterDictionary_1993-2021.csv\n",
            "\n",
            "  master_dictionary has 86,531 words.\n",
            "\n",
            "\n",
            "\n",
            "Runtime: 0:00:03.038235\n",
            "\n",
            "Normal termination.\n",
            "Sat Sep 30 17:07:18 2023\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subword tokenization using libraries like SentencePiece or Byte Pair Encoding (BPE) is useful when you want to tokenize text into subword units. This can be particularly helpful for languages with complex morphology or when dealing with out-of-vocabulary words. Here's how you can use these libraries:\n",
        "\n",
        "Using SentencePiece:\n",
        "SentencePiece is a popular subword tokenization library developed by Google. It's used for a wide range of natural language processing tasks.\n",
        "\n",
        "But this technique doesn't worked in our case. Accuracy and number of features fell dramatically, as soon as we applied it."
      ],
      "metadata": {
        "id": "HeoYkeAH3rLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a SentencePiece model for Tokenization\n",
        "!pip install sentencepiece\n",
        "import sentencepiece as spm\n",
        "spm.SentencePieceTrainer.train(input=(r\"/content/Drive/MyDrive/python/data.csv\"), model_prefix='mymodel', vocab_size=10000) # vocab_size=5000\n",
        "sp = spm.SentencePieceProcessor(model_file='mymodel.model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqe6zVVwPeM8",
        "outputId": "ba24aadc-5288-42d5-803a-bc7031eec6c4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "A8NsKSDV4q12"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2wklQSHh4xQv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell is dedicated for preprocessing and tokenization processes."
      ],
      "metadata": {
        "id": "LMb_h3O3fu0K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_oqnzngiVoA4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Custom Tokenization function\n",
        "def custom_tokenize(text):\n",
        "    # Regular expressions to tokenize financial-specific terms and symbols\n",
        "    # Example: Tokenize stock tickers like AAPL as single tokens\n",
        "    text = re.sub(r'([A-Z]+)', r'\\1 ', text)  # Add space after uppercase letters\n",
        "\n",
        "    # Example: Tokenize currency symbols (e.g., $, €) and percentages\n",
        "    text = re.sub(r'([$€%])', r' \\1 ', text)  # Convert currency symbols and percentages to standardized forms\n",
        "\n",
        "    # Tokenize numbers and symbols separately, and keep decimal numbers intact\n",
        "    text = re.sub(r'(\\d+\\.\\d+|\\d+)', r' \\1 ', text)  # Separate numbers and decimals\n",
        "    text = re.sub(r'([$€%])', r' \\1 ', text)  # Separate currency symbols and percentages\n",
        "\n",
        "    # Tokenize the text into words\n",
        "    words = text.split()\n",
        "\n",
        "    # Initialize lemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Apply lemmatization and lowercase, filter out single letters and common stopwords\n",
        "    tokens = [lemmatizer.lemmatize(word.lower()) for word in words if len(word) > 1 and word.lower() not in stopwords.words('english')]\n",
        "\n",
        "    # Filter out single characters and symbols\n",
        "    tokens = [word for word in words if len(word) > 1 or re.match(r'[$€%]', word)]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Function to convert numbers to text representations\n",
        "def convert_numbers_to_text(tokens):\n",
        "    converted_tokens = []\n",
        "    for token in tokens:\n",
        "        if token.replace('.', '', 1).isdigit():  # Check if it's a number\n",
        "            converted_token = num2words(token)  # Convert number to text (e.g., \"$1.50\" to \"one point five zero dollars\")\n",
        "            converted_tokens.append(converted_token)\n",
        "        else:\n",
        "            converted_tokens.append(token)\n",
        "    return converted_tokens\n",
        "\n",
        "# Data Preprocessing\n",
        "def preprocess_text(text):\n",
        "\n",
        "    # Define a regular expression pattern to remove URLs and punctuation marks\n",
        "    # pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+|[.,?!:;\"`(){}\\[\\]<>]'\n",
        "    # Remove URLs and punctuation marks\n",
        "    # cleaned_text = re.sub(pattern, '', text)\n",
        "\n",
        "    # Tokenization using custom tokenization function\n",
        "    # tokens = custom_tokenize(cleaned_text)\n",
        "    tokens = word_tokenize(text)\n",
        "    # tokens = sp.encode_as_pieces(text) # Tokenizing Using SentencePiece\n",
        "    # print(\"Tokens:\", tokens)\n",
        "\n",
        "    # Store the count of tokens before removing stopwords\n",
        "    # tokens_before_removal = tokens.copy()\n",
        "\n",
        "    # Remove stopwords\n",
        "    # stop_words = set(stopwords.words('english'))\n",
        "    # removed_words = [word for word in tokens_before_removal if word in stop_words]\n",
        "    # tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Calculate how many stopwords were removed\n",
        "    # removed_stopwords_count = len(tokens_before_removal) - len(tokens)\n",
        "    # print(f\"Removed {removed_stopwords_count} stopwords.\")\n",
        "    # print(f\"Removed words: {removed_words}\")\n",
        "\n",
        "    # stemmer = PorterStemmer()\n",
        "    # words = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    # Numerical handling (convert numbers to text representations)\n",
        "    tokens = convert_numbers_to_text(tokens)\n",
        "\n",
        "    # Join the words back to form preprocessed text\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "\n",
        "    # Print the tokens for debugging\n",
        "    # first_100_tokens = tokens[:100]\n",
        "    # print(first_100_tokens)\n",
        "\n",
        "    # Return the text and sentiment scores\n",
        "    return preprocessed_text\n",
        "\n",
        "# df['Sentence'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ELJfUIOvVQ0Q"
      },
      "outputs": [],
      "source": [
        "# Methods\n",
        "\n",
        "# Function to convert bytes to MB or GB\n",
        "def bytes_to_mb_or_gb(byte_value):\n",
        "    if byte_value >= 1024**3:  # GB\n",
        "        return f\"{byte_value / (1024**3):.2f} GB\"\n",
        "    elif byte_value >= 1024**2:  # MB\n",
        "        return f\"{byte_value / (1024**2):.2f} MB\"\n",
        "    else:  # bytes\n",
        "        return f\"{byte_value} bytes\"\n",
        "\n",
        "def train_and_evaluate_blended_classifier(classifier_combo, additional_info, num_classifiers):\n",
        "    print(f\"Blending the following classifiers: {classifier_combo}\")\n",
        "\n",
        "    start_time = time.time()  # Record the start time\n",
        "    start_time = time.time()\n",
        "    # Get CPU information\n",
        "    num_cores = multiprocessing.cpu_count()\n",
        "    processor_type = platform.processor()\n",
        "    # Get OS information\n",
        "    os_name = platform.system()+'('+platform.release()+')'\n",
        "    # Get RAM information\n",
        "    ram_total = os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES')\n",
        "    ram_total_mb = ram_total / (1024**2)\n",
        "    # Calculate CPU and RAM resources before training\n",
        "    cpu_before = psutil.cpu_percent()\n",
        "    available_memory_before = psutil.virtual_memory().available\n",
        "\n",
        "################################################################################\n",
        "    # Create the voting classifier with the selected combination\n",
        "    selected_classifiers = [(name, classifiers[name]) for name in classifier_combo]\n",
        "    voting_classifier = VotingClassifier(estimators=selected_classifiers, voting='hard')\n",
        "\n",
        "    # Train the voting classifier\n",
        "    voting_classifier.fit(X_train_tfidf_dense, y_train)\n",
        "\n",
        "    # Make final predictions on the test data using the voting classifier\n",
        "    final_predictions = voting_classifier.predict(X_test_tfidf_dense)\n",
        "\n",
        "    # Calculate evaluation metrics for the blended model\n",
        "    accuracy = accuracy_score(y_test, final_predictions)\n",
        "    precision = precision_score(y_test, final_predictions, average='weighted', zero_division=1)\n",
        "    recall = recall_score(y_test, final_predictions, average='weighted')\n",
        "    f1 = f1_score(y_test, final_predictions, average='weighted')\n",
        "    cm = confusion_matrix(y_test, final_predictions)\n",
        "\n",
        "    # Calculate Cohen's kappa\n",
        "    kappa = cohen_kappa_score(y_test, final_predictions)\n",
        "\n",
        "################################################################################\n",
        "\n",
        "    end_time = time.time()  # Record the end time\n",
        "    execution_time = end_time - start_time  # Calculate the execution time\n",
        "    # Calculate CPU and RAM resources after training\n",
        "    cpu_after = psutil.cpu_percent()\n",
        "    available_memory_after = psutil.virtual_memory().available\n",
        "\n",
        "    # Calculate resource usage during this iteration\n",
        "    cpu_usage = max(cpu_after - cpu_before, 0)\n",
        "    memory_usage = max(available_memory_after - available_memory_before, 0)  # Ensure non-negative value\n",
        "    memory_usage_str = bytes_to_mb_or_gb(memory_usage)\n",
        "\n",
        "    print(f\"CPU Usage: {cpu_usage:.2f}%\")\n",
        "    print(f\"Memory Usage: {memory_usage_str}\")\n",
        "\n",
        "    # Store the results in the list as a dictionary\n",
        "    results_dict = {\n",
        "        'Blended Classifiers': classifier_combo,\n",
        "        'Accuracy': accuracy,\n",
        "        'Kappa': kappa,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-Score': f1,\n",
        "        'Confusion Matrix': cm,\n",
        "        'Execution Time (s)': execution_time,\n",
        "        'Total Classifiers': additional_info['Total Classifiers'],\n",
        "        'Total Features': additional_info['Total Features'],\n",
        "        'Training Data Size': additional_info['Training Data Size'],\n",
        "        'Test Data Size': additional_info['Test Data Size'],\n",
        "        'Random State': additional_info['Random State'],\n",
        "        'Preprocessing': additional_info['Preprocessing'],\n",
        "        'SMOTE': additional_info['SMOTE'],\n",
        "        'Total CPU Cores': num_cores,\n",
        "        'CPU Usage (%)': cpu_usage,\n",
        "        'Total RAM': ram_total_mb,\n",
        "        'Memory Usage': memory_usage_str,\n",
        "        'Processor Type': processor_type,\n",
        "        'OS': os_name\n",
        "    }\n",
        "    print(results_dict)\n",
        "\n",
        "\n",
        "    return results_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec:\n",
        "We can use Word2Vec in our code to learn word embeddings from your text data. To do this, we'll need to use a library like Gensim in Python, which provides an easy way to train Word2Vec models."
      ],
      "metadata": {
        "id": "HuE9L_Zpn2MP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-f2rfrDTso-i",
        "outputId": "2ffb20de-a11c-4abf-c043-b8e1546f0a30"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Sentence', 'Sentiment', 'Preprocessed_Sentence'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating our own word2vec model\n",
        "\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "df['Preprocessed_Sentence'] = df['Sentence'].apply(preprocess_text)\n",
        "# print(df.columns)\n",
        "corpus = df['Preprocessed_Sentence'].apply(lambda x: x.split())  # Assuming each sentence is a space-separated list of words\n",
        "\n",
        "# Train Word2Vec model with more epochs\n",
        "word2vec_model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1, sg=0, epochs=10)\n",
        "\n",
        "# Monitor Loss: You can track the loss (negative log likelihood) during training using the get_latest_training_loss() method of the Word2Vec model. This method returns the loss for the most recent batch of training data:\n",
        "# Check the latest training loss\n",
        "latest_loss = word2vec_model.get_latest_training_loss()\n",
        "print(f\"Latest Training Loss: {latest_loss}\")\n",
        "# The loss should decrease over time during training. If it remains constant or increases significantly, it may indicate an issue with your data or training parameters.\n",
        "\n",
        "# Get the list of words in your Word2Vec model's vocabulary\n",
        "vocabulary_words = word2vec_model.wv.index_to_key\n",
        "# Print the first 10 words as an example\n",
        "print(\"First 10 words in the vocabulary:\")\n",
        "print(vocabulary_words[:10])\n",
        "\n",
        "# Test Word Similarity: After training, you can test whether the model has learned meaningful word embeddings by checking word similarity. Gensim's wv attribute provides access to the word vectors. You can use the similarity method to check the similarity between words:\n",
        "# Check word similarity between two specific words\n",
        "# word1 = \"the\"\n",
        "# word2 = \"and\"\n",
        "# similarity_score = word2vec_model.wv.similarity(word1, word2)\n",
        "# print(f\"Similarity between '{word1}' and '{word2}': {similarity_score}\")\n",
        "# Replace \"word1\" and \"word2\" with words from your vocabulary to check their similarity. Higher similarity scores indicate that the model has learned meaningful word representations.\n",
        "\n",
        "# Save the model to a file\n",
        "word2vec_model.save(\"word2vec_model.bin\")\n",
        "\n",
        "# Load the model from the file\n",
        "loaded_model = Word2Vec.load(\"word2vec_model.bin\")\n",
        "\n",
        "loaded_model\n",
        "\n",
        "# Save the trained model to a local file\n",
        "# word2vec_model.save('word2vec_model.bin')\n",
        "\n",
        "# import shutil\n",
        "\n",
        "# # Specify the destination folder in your Google Drive\n",
        "# destination_folder = '/content/Drive/MyDrive/python/financial_sentiment_analysis/'\n",
        "\n",
        "# # Move the trained model to Google Drive\n",
        "# shutil.move('word2vec_model.bin', destination_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYtxGybWn4em",
        "outputId": "89431f5e-0160-45e4-ddfc-c0d8ffa1c700"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latest Training Loss: 0.0\n",
            "First 10 words in the vocabulary:\n",
            "['.', 'the', ',', 'and', 'of', 'in', 'to', 'point', 'two', 'a']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gensim.models.word2vec.Word2Vec at 0x7bc523387d30>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fU_reZlscOfX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "444b9d55-22d6-4268-a04d-94ded4a221f7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-d57424c6a494>\u001b[0m in \u001b[0;36m<cell line: 88>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0my_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word2vec_model.bin\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m# Iterate through the documents to create Word2Vec feature vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, rethrow, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1951\u001b[0m         \"\"\"\n\u001b[1;32m   1952\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1953\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1954\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1955\u001b[0m                 \u001b[0mrethrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_lifecycle_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m_load_specials\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1972\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns_exponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1973\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'index_to_key'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1974\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_cum_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# rebuild cum_table from vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1975\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'corpus_count'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1976\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mmake_cum_table\u001b[0;34m(self, domain)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0mtrain_words_pow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m             \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vecattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'count'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m             \u001b[0mtrain_words_pow\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns_exponent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0mcumulative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vecattr\u001b[0;34m(self, key, attr)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpandos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vecattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \"\"\"Get attribute value associated with given key.\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Vectorizing, Balancing, Extracting Features and Splitting the Data.\n",
        "\n",
        "# Constants\n",
        "PREPROCESSING_FLAG = 1\n",
        "SMOTE_FLAG = 1\n",
        "# MAX_FEATURES_LIMIT = 15000\n",
        "TEST_SIZE = 0.2\n",
        "RAND_STATE = 42\n",
        "\n",
        "# Apply data preprocessing to the 'Sentence' column and create 'Preprocessed_Sentence' column\n",
        "df['Preprocessed_Sentence'] = df['Sentence'].apply(preprocess_text)\n",
        "\n",
        "# Prepare the data\n",
        "if PREPROCESSING_FLAG == 1:\n",
        "  X = df['Preprocessed_Sentence']\n",
        "else:\n",
        "  X = df['Sentence']\n",
        "\n",
        "y = df['Sentiment']\n",
        "\n",
        "################# TESTING #####################\n",
        "# Initialize lists to store sentiment scores for each document\n",
        "positive_scores = []\n",
        "negative_scores = []\n",
        "uncertainty_scores = []\n",
        "litigious_scores = []\n",
        "strong_modal_scores = []\n",
        "weak_modal_scores = []\n",
        "constraining_scores = []\n",
        "\n",
        "# Iterate through your dataset (replace df with your actual DataFrame)\n",
        "for text in df['Preprocessed_Sentence']:\n",
        "    tokens = text.split()\n",
        "    # print(tokens)\n",
        "\n",
        "    # Initialize sentiment scores for each document\n",
        "    sentiment_scores = {\n",
        "        'positive': 0,\n",
        "        'negative': 0,\n",
        "        'uncertainty': 0,\n",
        "        'litigious': 0,\n",
        "        'strong_modal': 0,\n",
        "        'weak_modal': 0,\n",
        "        'constraining': 0\n",
        "    }\n",
        "\n",
        "    # Calculate sentiment scores for each token in the document\n",
        "    for token in tokens:\n",
        "        token = token.upper()  # Convert token to uppercase for matching\n",
        "        if token in master_dictionary:\n",
        "            entry = master_dictionary[token]\n",
        "            for sentiment_category in sentiment_scores.keys():\n",
        "                if getattr(entry, sentiment_category) > 0:\n",
        "                    sentiment_scores[sentiment_category] += 1\n",
        "\n",
        "    # Append sentiment scores to their respective lists\n",
        "    positive_scores.append(sentiment_scores['positive'])\n",
        "    negative_scores.append(sentiment_scores['negative'])\n",
        "    uncertainty_scores.append(sentiment_scores['uncertainty'])\n",
        "    litigious_scores.append(sentiment_scores['litigious'])\n",
        "    strong_modal_scores.append(sentiment_scores['strong_modal'])\n",
        "    weak_modal_scores.append(sentiment_scores['weak_modal'])\n",
        "    constraining_scores.append(sentiment_scores['constraining'])\n",
        "\n",
        "# Add sentiment score columns to your DataFrame\n",
        "df['Positive_Score'] = positive_scores\n",
        "df['Negative_Score'] = negative_scores\n",
        "df['Uncertainty_Score'] = uncertainty_scores\n",
        "df['Litigious_Score'] = litigious_scores\n",
        "df['Strong_Modal_Score'] = strong_modal_scores\n",
        "df['Weak_Modal_Score'] = weak_modal_scores\n",
        "df['Constraining_Score'] = constraining_scores\n",
        "\n",
        "# Print positive and negative scores for the first few documents (adjust the range as needed)\n",
        "# for i in range(50):  # Print scores for the first 50 documents as an example\n",
        "#     print(f\"Document {i + 1} - Positive Score: {positive_scores[i]}, Negative Score: {negative_scores[i]}\")\n",
        "\n",
        "################# Feature Extraction #####################\n",
        "\n",
        "# Create a TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer() # max_features=MAX_FEATURES_LIMIT\n",
        "X_train = tfidf_vectorizer.fit_transform(X)\n",
        "\n",
        "# Convert the labels to numerical values using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "loaded_model = Word2Vec.load(\"word2vec_model.bin\")\n",
        "\n",
        "# Iterate through the documents to create Word2Vec feature vectors\n",
        "word2vec_features = []\n",
        "\n",
        "for doc in df['Preprocessed_Sentence']:\n",
        "    tokens = doc.split()\n",
        "    word2vec_embeddings = []\n",
        "\n",
        "    for token in tokens:\n",
        "        if token in loaded_model.wv:\n",
        "            word2vec_embeddings.append(loaded_model.wv[token])\n",
        "\n",
        "    # Calculate the average Word2Vec embedding for the document\n",
        "    avg_embedding = sum(word2vec_embeddings) / len(word2vec_embeddings) if word2vec_embeddings else [0.0] * loaded_model.vector_size\n",
        "\n",
        "    # Convert the average Word2Vec embedding to a dense array\n",
        "    avg_embedding_dense = [float(x) for x in avg_embedding]\n",
        "\n",
        "    word2vec_features.append(avg_embedding_dense)\n",
        "\n",
        "# Convert the list of Word2Vec features to a numpy array\n",
        "X_word2vec = np.array(word2vec_features)\n",
        "\n",
        "# Merge TF-IDF, sentiment scores, and Word2Vec features\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "X_features = hstack((X_train, df[['Positive_Score', 'Negative_Score', 'Uncertainty_Score', 'Litigious_Score', 'Strong_Modal_Score', 'Weak_Modal_Score', 'Constraining_Score']].values), format='csr')\n",
        "X_combined = hstack((X_features, X_word2vec), format='csr')\n",
        "\n",
        "# Print the number of features added\n",
        "print(f\"Number of features added: {X_combined.shape[1]}\")\n",
        "\n",
        "# Balancing the Dataset\n",
        "smote = SMOTE(sampling_strategy='auto', random_state=RAND_STATE) # random_state=RAND_STATE\n",
        "X_train_balanced, y_train_balanced = smote.fit_resample(X_combined, y_encoded)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train_balanced, y_train_balanced, test_size=TEST_SIZE, random_state=RAND_STATE) # random_state=RAND_STATE\n",
        "\n",
        "# Convert X_train_tfidf to a dense numpy array\n",
        "X_train_tfidf_dense = X_train.toarray()\n",
        "\n",
        "# Convert X_test_tfidf to a dense numpy array\n",
        "X_test_tfidf_dense = X_test.toarray()\n",
        "\n",
        "if np.any(X_train_tfidf_dense < 0):\n",
        "    # Handle or debug the presence of negative values in TF-IDF features\n",
        "    print(\"Negative values found in TF-IDF features.\")\n",
        "\n",
        "# X_train_tfidf_dense\n",
        "# y_encoded\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hBcE7dNO5nt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "T9NgL91lVsL7"
      },
      "outputs": [],
      "source": [
        "individual_mode = False\n",
        "\n",
        "# Initialize classifiers as base models\n",
        "classifiers = {\n",
        "    # 'Logistic Regression': LogisticRegression(max_iter=1000),\n",
        "    'SDG': SGDClassifier(),\n",
        "    # 'Random Forest': RandomForestClassifier(max_depth=10),\n",
        "    'Support Vector Machine': SVC(),\n",
        "    # 'Naive Bayes': MultinomialNB(),\n",
        "    #  'Decision Tree': DecisionTreeClassifier(),\n",
        "    #  'Bagging': BaggingClassifier(),\n",
        "    # 'Gaussian Naive Bayes': GaussianNB(),\n",
        "    # 'Extreme Gradient Boosting (XGBoost)': XGBClassifier()\n",
        "\n",
        "}\n",
        "\n",
        "# Initialize blended_results_list\n",
        "results_list = []\n",
        "# Initialize blended_results_dict\n",
        "results_dict = {}\n",
        "# Initialize an empty list to store the rows of the results\n",
        "results_rows = []\n",
        "# Additional Information\n",
        "# Additional information about the code\n",
        "additional_info = {\n",
        "    'Total Classifiers': len(classifiers),\n",
        "    'Total Features': {X_combined.shape[1]},\n",
        "    # 'Total Features': len(tfidf_vectorizer.get_feature_names_out()),\n",
        "    'Training Data Size': len(y_train),\n",
        "    'Test Data Size': len(X_test_tfidf_dense),\n",
        "    'Random State': RAND_STATE,  # Replace with the actual random_state value used in train_test_split\n",
        "    'Preprocessing': PREPROCESSING_FLAG,\n",
        "    'SMOTE': SMOTE_FLAG\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cx29jTDPZkQ-",
        "outputId": "70aeef69-cf2a-4fa8-ddad-628fcaf1dc0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blending the following classifiers: ('SDG', 'Support Vector Machine')\n",
            "CPU Usage: 34.50%\n",
            "Memory Usage: 0 bytes\n",
            "{'Blended Classifiers': ('SDG', 'Support Vector Machine'), 'Accuracy': 0.7603833865814696, 'Kappa': 0.6400912228849344, 'Precision': 0.7798262949382323, 'Recall': 0.7603833865814696, 'F1-Score': 0.7569033531192514, 'Confusion Matrix': array([[587,  30,  14],\n",
            "       [147, 456,  29],\n",
            "       [ 97, 133, 385]]), 'Execution Time (s)': 795.9846560955048, 'Total Classifiers': 2, 'Total Features': {11199}, 'Training Data Size': 7512, 'Test Data Size': 1878, 'Random State': 42, 'Preprocessing': 1, 'SMOTE': 1, 'Total CPU Cores': 2, 'CPU Usage (%)': 34.49999999999999, 'Total RAM': 12982.625, 'Memory Usage': '0 bytes', 'Processor Type': 'x86_64', 'OS': 'Linux(5.15.120+)'}\n",
            "             Blended Classifiers  Accuracy     Kappa  Precision    Recall  \\\n",
            "0  (SDG, Support Vector Machine)  0.760383  0.640091   0.779826  0.760383   \n",
            "\n",
            "   F1-Score                                 Confusion Matrix  \\\n",
            "0  0.756903  [[587, 30, 14], [147, 456, 29], [97, 133, 385]]   \n",
            "\n",
            "   Execution Time (s)  Total Classifiers Total Features  ...  Test Data Size  \\\n",
            "0          795.984656                  2        {11199}  ...            1878   \n",
            "\n",
            "   Random State  Preprocessing  SMOTE  Total CPU Cores  CPU Usage (%)  \\\n",
            "0            42              1      1                2           34.5   \n",
            "\n",
            "   Total RAM  Memory Usage Processor Type                OS  \n",
            "0  12982.625       0 bytes         x86_64  Linux(5.15.120+)  \n",
            "\n",
            "[1 rows x 21 columns]\n",
            "Results have been appended to /content/Drive/MyDrive/python/results/blended_classifiers/lm_2023-09-30_blended_classifier_results.csv in Google Drive.\n"
          ]
        }
      ],
      "source": [
        "# Evaluating\n",
        "\n",
        "for r in range(2, len(classifiers) + 1):\n",
        "    combinations_list = list(combinations(classifiers, r))\n",
        "\n",
        "    # Iterate through each combination and call the train_and_evaluate_blended_classifier method\n",
        "    for combo in combinations_list:\n",
        "        num_classifiers = len(combo)  # Get the number of classifiers in the current combination\n",
        "        results = train_and_evaluate_blended_classifier(combo, additional_info, num_classifiers)\n",
        "        results_list.append(results)\n",
        "\n",
        "# Sort the blended results based on accuracy in descending order\n",
        "sorted_results = sorted(results_list, key=lambda x: (x['Accuracy'], x['Precision'], x['Recall'], x['F1-Score']), reverse=True)\n",
        "\n",
        "# Save the sorted_results and additional information to a DataFrame\n",
        "sorted_results_df = pd.DataFrame(sorted_results)\n",
        "print(sorted_results_df)\n",
        "\n",
        "# Set the time zone to Pakistan Standard Time (PST)\n",
        "pakistan_tz = pytz.timezone('Asia/Karachi')\n",
        "\n",
        "# Get the current date in Pakistan's time zone\n",
        "current_date = datetime.now(pakistan_tz).strftime('%Y-%m-%d')\n",
        "\n",
        "# Append the new results to the existing file or create a new file if it doesn't exist\n",
        "file_name = f\"lm_{current_date}_blended_classifier_results.csv\"\n",
        "file_path = f\"/content/Drive/MyDrive/python/results/blended_classifiers/{file_name}\"\n",
        "\n",
        "try:\n",
        "    if os.path.exists(file_path):\n",
        "        existing_results_df = pd.read_csv(file_path)\n",
        "        final_df = pd.concat([existing_results_df, sorted_results_df], ignore_index=True)\n",
        "    else:\n",
        "        final_df = sorted_results_df\n",
        "\n",
        "    final_df.to_csv(file_path, index=False)\n",
        "    print(f\"Results have been appended to {file_path} in Google Drive.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while saving the file: {str(e)}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9YrQSYFaIyBDzKtXLrYrs",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}